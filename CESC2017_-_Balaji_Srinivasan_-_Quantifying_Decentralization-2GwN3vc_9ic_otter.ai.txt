Balaji Srinivasan  0:07  
So I've got a very kind of simple talk today, which is about a metric for actually quantifying decentralization. And the reason this is important is that many different kinds of attacks on digital currencies and blockchains, and so on, are, you know, in the form of, Hey, you know, how many folks do we need in order to compromise this in order to centralize it on this dimension? Or that dimension? Can we centralized mining and do a 51% attack? Can we knock down exchanges? Can we, you know, corrupt nodes, and, you know, launch Sibyl attacks, and so on. And I've got a simple, imperfect, but interesting and universal measure of how decentralized a system is, and you can use it to identify decentralization bottlenecks. And just as motivation for this, the reason this is important is until you have a quantitative measure, even an imperfect one, you can't say whether one system is more or less decentralized than another. And you certainly can't say this is the optimal level of decentralization given scarce resources. So a metric is the first step towards trying to make you know, our conversation on digital decentralization and blockchains. A little more scientific. Okay, but that is motivation. Um, here's my background. You know, I'm 21, CEO and partner at Andreessen Horowitz, who you might have might have heard of us. And so as I mentioned, today, we're going to talk about a simple metric for quantifying decentralization. I've got funding for a decentralization dashboard. Actually, I got a couple engineers working on this. When this is live, we'll have it up at decentralized comm which is pretty good, good URL. And we've got also a new token, which is optimizing on least one dimension of decentralization, which is ownership coming out at social token calm, but most of it's going to be about this this first bit. So when we talk about quantifying decentralization, everybody agrees that decentralization is important. And that, you know, typically, the more decentralized system system is, the more censorship resistant it is. But to say that it's more or less, you need to have a quantitative metric. And just to kind of level set and kind of emphasize how important decentralization is, you know, when Satoshi introduced Bitcoin in the second sentence, like one of the key selling points was it's completely decentralized, right? And zabo you know, decentralization is what allows Bitcoin to subsume army. And, you know, vitalik, on decentralization, you know, viewed as a block chains entire raison d'etre, you know, Adam Ludwin of chain, you know, basically, it's pretty clear that most of folks in the space, talk about decentralization as if it is the sine qua non of what makes blockchains blockchains. So, it would be great if we had some quantitative measures of this incredibly important thing. And, you know, so the reason that quantification is important is if any of you here is familiar with like optimization theory, or the concept of like a loss function is, you know, until you can actually have a loss function, you can't plug in all the tools of, you know, optimization theory, whether the, you know, that that function is convex or non convex, whether this set over which you're optimizing is convex or non convex, you need to at least have a function before you can start approximating things. And what I'm going to propose through this, I'm going to motivate a function and then show you that function, which we call the minimum Nakamoto coefficient. Okay, so that's motivation, let's just introduce a few, you know, kind of concepts leading up to this measure. So first is this idea of the Lorenz curve and Gini coefficient, right. So the Lorenz curve, it's used by economists to measure inequality. And typically inequality is a concern of one political faction and centralization, a concern of another, but they're actually quite similar. That is to say, inequality of money and centralization of power, you know, are not, you know, synonymous, but a lot of the measures are similar. And so let's review these, these two ideas of Lorenz curve and Gini coefficient. So if you have a population where everybody has exactly the same amount of money, or power, um, you'd have a uniform distribution, it'd be flat over here. And if you sum it, if you integrate it, if you take the CDF, you have like this triangle like this, the more skewed the distribution of money or power, or what have you, the more quote unequal, the more centralized. And so you know, perfect decentralization would be everybody has exactly the same amount of power. Perfect centralization is one party has all the power, okay? So it's kind of interesting, you can actually start, you know, thinking about this in this sort of, you know, systematic way. And a second kind of concept is, if we take a decentralized system, we can break it up into subsystems. And all these subsystems, by the way, are for illustration only, I'm not arguing This is the canonical decomposition or anything like that. But it is, it is one way of decomposing things. And, you know, then we can, you know, kind of go and review, right.

So, you know, for illustration, there's, you know, six subsystems of Bitcoin that I've just chosen here. And you can add or subtract different subsystems according to your threat model. So for example, with mining, you can look at that as a subsystem and we can analyze it centralization on the basis of its block reward. With plants. We can see Okay, have these different code bases that have been, you know, kind of deployed, you know, among different nodes? What percentages, you know, run, let's say Bitcoin Core versus, you know, BTC, or rather than BTC. D and other kinds of clients, among a particular codebase? How centralized is development? Is it 1000 developers all doing it? Or is it just a couple? Or, you know, is it something in between? With exchanges? How centralized are they big volume? If you take down one exchange? Do you knock out the entire, you know, trading volume of this coin? With nodes? Can a country just go and block, you know, all of these nodes are they spread out between countries, and with ownership how concentrated as well as just that one person owns it, or does everybody on it and a lot of folks have a stake in it. And again, I'm not arguing each of these are of equal weight, or anything like that, I'm just giving an example of how you can take a subsystem, or you can take a public blockchain, decompose it into these decentralized subsystems, then analyze the centralization of each one. The reason I went with these six, just as illustrative for illustrative purposes, is each of these has been at one point or another, the subject of a political dispute within the digital currency community over how centralized or decentralized is mining, a centralized development is centralized, you'll always hear these things bandied around I take no position on that, just trying to give some measure. So we can at least, you know, kind of quantify, you know, the argument. Okay, so for each of these subsystems, we went ahead and we converted, you know, we calculated the Lorenz curves and Gini coefficients, so here it is, for mining and for clients, and for developers. So for example, you know, it's very skewed by by clients, because one client, you know, Bitcoin Core is actually responsible for the vast majority of nodes on the network, that may be a good thing in your mind, you may decide that this is not an important graph, and you can just throw it away. But just the measure of it shows it's fairly, fairly centralized in terms of how nodes are running it. For exchanges for nodes, and for owners, we also calculate all this stuff, and all the stuff is online. If you Google quantifying decentralization, there's a blog post we did. It's at news.twit.co, you can you can see that blog post. So calculated this for Bitcoin. And we did the same thing for aetherium. We looked at, you know, the decentralization for mining and for clients and for developers. So for example, you know, for developers, we're looking at death, you know, the primary reference client, we're asking, okay, how many commits to the first guy make, you know, of all the developers versus the second guy versus the third guy in terms of their rank ordered distribution of commits, we find that the first few folks made actually a fairly high percentage of the number of commits. We also looked at their change, decentralization, and no decentralization and owner decentralization for aetherium. This is useful, these kinds of graphs are interesting. But one of the issues is if I say a Gini Coefficient of point eight, five, or point seven, six, it's hard to actually really wrap your heads around what that means. And so we propose something which tracks with that, but it's a little more intuitive, which we call not the Gini coefficient. But the maximum Gini coefficient, which is a decentralized system is only as decentralized as its least decentralized subsystem, right? So if you take all of the Gini coefficients together, and all these aetherium energy computers together, you can actually say, Okay, if I take the maximum of them, that's a subsystem, which is the most centralized in both of them. For both Bitcoin and aetherium. The most single most centralized component, at least by this crude measure, which I acknowledge is crude, I acknowledge it's imperfect. But this crude measure is actually clients, where basically, you know, you're looking at clients and saying, Okay, um, you know, the top client is actually, you know, pretty centralized in the sense of the vast majority of nodes never could really just running one client. It's not, it's not like web browsers, where you've got, you know, Firefox, and Safari and Chrome and IE, all with a reasonable amount of share. Um, it's really mostly just just one major client. So this is this is one way of trying to summarize it. And the utility of this is we can identify even crudely a postulated decentralization bottleneck, we can say, given scarce resources, we'd like to, you know, decentralize this more. And allocating more effort in some other subsystem may not be as valuable if and only if you consider this to be a useful metric. If not, you can throw that away and take your maximum over the remaining columns. Okay. So So this is, you know, kind of a step forward in the sense that we can start to, you know, get our heads around this idea, we take this complex idea of a of a Bitcoin or theorem, this, this decentralized system, break into subsystems, measure, decentralization coefficients Gini coefficients across each and then try to calculate the maximum, but even still point 915 Genie, it's not super intuitive, can we do even better? And we've got, I think, something which is a little more intuitive still, which we call the minimum Nakamoto coefficient, right. And the idea here is if we go back to the Lorenz curve, and now we ask something that's a little more, you know, like, like an integer, right? We say,

how many people or how many entities Do you need to capture in order to capture 51% of the money or the power, right? And so in this case, for example, you know, with 100 people, you need to capture at least 50 people in order to get half the power, whereas here, it's much more skewed and you'd have to capture eight people in order to capture 51% of the power that captures an intuitive, integer kind of measure of how centralized system is here. Eight people control more than 51% here, it's like so even that you need to get 50 to get 51%. Right? Okay. So if we counted the Nakamoto coefficient for the systems is basically like for miners, you'd get to more than 51% of block rewards. One client, you'd get to more than 51% of the Bitcoin, node codebase. In terms of clients, it require five developers of Bitcoin Core to get to more than 51% of commits, it would take five exchanges to get to more than 51% of exchange volume, it would take three different countries to get to more than 51% of nodes. And it would take about 456 holders to get more than 51% of Bitcoin for large holders of Bitcoin. That's an important thing. Because if you include everybody in the world, most people in the world have zero. And so then you're going to have like a trivial, you know, coefficient of point nine, nine. So you can decide whether or not you consider that one important. Um, but that's just something where people always ask for it. So I include that that measure, we can do the same thing for aetherium. So at least over this 24 hour snapshot, there were two miners that had the majority, the block reward, this varies actually a great deal on the with clients, again, guess you know, one client has the majority more than 51% of nodes running it. With, there's two developers that have more than 51% of get commits. Again, at the time of this snapshot, five of changes have more than 15% of Ethereum volume, four countries had more than 51% of aetherium nodes, and about 72 holders had more than 50% of Ethereum, conditional on them having a certain large amount of aetherium. Again, otherwise, without that boundary condition you don't get, you don't get a very informative thing. And the the thing that's interesting about this is we can calculate very similar tables, where we asked, okay, for each of these, you know, kind of columns over here, how many folks? How many entries Do we need to capture in order to capture it, we see that basically, clients are, you know, the bottleneck in both cases where if you capture one client code base, you can centralize network and other subsystems are less centralized by comparison.

And, you know, there's obvious objections, okay? On this is an imperfect metric, in the same way that if you go and take 1000 images of cats, and label them zero or one, you haven't necessarily captured the essence of a cat. But it's while imperfect, still useful, because you can start training machine learning algorithms, in the same way, I would argue that while the choice of subsystem matters a lot and subsystems, some subsystems may not be important. You can you can necessarily, you can nevertheless, deal with that. And let me let me go into little more detail here. so obvious objections, right. So first, the metric is sensitive to subsystem definitions and subsystem definitions must map to attacks to be useful, right? So, for, for example, over here, what someone could say is alright, Balaji, if you if you take the Nakamoto coefficient, and you call it the metric, and you say, how many people or clients do I need to compromise it? Well, in a theorem partisan might argue that the theorem clients are actually more decentralized, because get and parody are completely different code bases, whereas the most popular Bitcoin clients are actually, you know, typically forks of Bitcoin Core. And so actually, a theorem is more decentralized by that metric. And I say fine, you know, just redefine the subsystem that way. But at least we're now starting to argue about something quantitative, right? It's not completely abstract. It's not a religious kind of thing. This is more decentralized versus that it is this more decentralized by this measure. And so at least we made some forward progress. Okay, so that's, number one. I agree. It's sensitive to subsystem definitions. But a subsystem definition is way better than just an ambiguous and abstract thing. Number two, someone may argue, for example, here, Satoshi, saying, you know, I don't believe a second compatible implementation of Bitcoin will ever be a good idea. So Bitcoin partisan may argue, you know, having one client is not a bad thing. Fine. Okay. So that's the second kind of thing if you want to throw out a particular subsystem, okay. If you don't consider that something that is a centralization bottleneck, if you consider that a necessary evil. Go ahead. You may think dev centralization is not an issue. But mining is, again, at least we can start making this quantitative, you throw out that row of the table. And then third, someone might say, Well, you know, if you're saying it's a Nakamoto coefficient is the number of people or entities that you need to get 51% of the wealth or power 51% may not be the upper threshold. And so obviously, what you can do is respond to that. And you can say, yeah, just do a modified Nakamoto coefficient, which says, How many people don't need to control 25% or 75%, how many people don't need to control to launch this particular attack against network. So for example, here, you know, you've got selfish mining this 25% attack, even given these objections, I still believe that this coefficient moves us in the right direction. Because what you can do is pick attacks that you think likely, and the minimum Nakamoto coefficient, you know, basically directionally starts to buffer you against those attacks if it's high, right. So, you know, for example, in mining, if your measure is hash rate across countries, and a particular country, let's say China nationalizes or requires mining on then, you know, if your minimum Nakamoto coefficient is like five, well, you just dropped one one country turned off mining, but the other four, you know, you need to capture the other four to get 51% of mining so you've decentralized it. Second example, if you take clients and you talk about market share across nodes, and there's a bug in one client's code base, like you know, for example, parody, Bitcoin unlimited Bitcoin Core all had significant bugs this year. Again, those engineers are great. I'm not saying any engineering engineers is not good. But those clients did have bugs. And those bugs were user visible. If one client has a bug, but your Nakamoto coefficient is like four? Well, again, you know, most of the network is unaffected, right? Because you needed to mess up a bunch of different clients to capture more than 50% 51% of nodes. third example, right? So commits across developers. I don't know if people remember this, but 20 years ago, on there was a crypto wars, you know, the one, you know, crypto war one, that wage was waged in the US, and guys like Phil Zimmerman were prosecuted for PGP, right. So you do want to have a Nakamoto coefficient on a particular Git repository to be fairly high. So if any one developer goes down, it's not 51% of commits, right? So you have three, four or five developers who all have knowledge of the codebase. So one guy gets knocked down, he gets jailed, he gets prosecuted. And you know, the other four can still carry on fourth example, volume across country. So we just saw this one, you know, China got shut down. We're very lucky that and not just lucky, you know, happy, you know, fortunate that digital currencies exchanges were spread out worldwide, that other exchanges could pick up the slack. And it really didn't, didn't miss a beat, you know, things have shifted over to Japan and South Korea, in other countries, even though China shut down exchanges, that was very lucky for us that, you know, the Nakamoto coefficient could evolve. Its distribution across server entities for for nodes, right. So if Amazon Web Services or Google Cloud shut down nodes, and we've already seen that, you know, you know, domains are getting yanked, you know, the Spanish government has shut down, you know, 100,000 CIT demands, it may be at some point that Amazon or Google Cloud decide that these nodes violate the acceptable use policy, if so, it behooves us to have a Nakamoto coefficient across server entities, that's much higher than two or three, there's a lot of different entities besides Amazon and Google that, you know, are running nodes. And finally, distribution across individuals on this one is a little more speculative. But what we don't want is to have something where it's an extreme asymmetry, where there's a political attack by those without digital currency, to support seizure from those who have it right. And so like in Venezuela, or something like that, we've seen kinds of attacks like that. So this is something where, you know, you can argue about how equal the distribution needs to be. But the more equal it is, the less likely there is to be a political attack. So, now, given these hypothesize attack types, you can choose your subsystems. If you think a particular row is not that important, just discard that. Okay. And, you know, that's basically the kind of dashboard we're gonna set up, where you can just check box and uncheck, and you can, you can do it. As an analogy, if anybody remembers, or has used like the computer language benchmarks game, they know that benchmarks are imperfect. A benchmark does not say a computer language is fast, it says it is fast on this benchmark. In the same way, the Nakamoto coefficient doesn't say, this coin or this token is decentralized. It says it is decentralized on this benchmark. That is to say it has 1000 different miners who you need to compromise to get to 51%. Hence, it's decentralized on this benchmark. So no one benchmark determines that a language is fast, but a combination gives useful results. And the same thing, you know, with machine learning, there's no one thing you can say to determine whether something's a hot dog or not. But a conversation, you know, combination gives useful results if you guys have seen this, right. So imperfect quantification can lead to useful results. Finally, you know, I think at least this is a little bit philosophical. But I think decentralization probably won't be accomplished through one coin alone, you're going to have different strategies for decentralization and across coin strategy, where even if one coin is compromised, if the other ones maintain exchange rates, that one may be more robust to attacks and different people who are founders of different coins and tokens have theses on different kinds of attacks, they think of as plausible, and they optimize them for robustness to that particular vector. Okay, so just in summary, we propose a simple metric of decentralization, this Nakamoto coefficient, if you want the equations from it, I went relatively equation light. But we've got equations in this blog post called quantifying decentralization. It tracks with our intuitive notions, given a list of subsystems, you can start with attacks, get subsystems and then measure. And you can put this into dashboards and objective functions. What we're doing next, as I mentioned, we're gonna fund a dev to do this decentralized comm are already working on this. So you can get a real time dashboard of how centralized or decentralized various coins and tokens are on these benchmarks. And they'll be open source. So if you quibble with a particular benchmark, or think you shouldn't have this or that, you can come back and refresh it, and you can flip all the settings set your coin is the most decentralized, dammit. And just like you can go to the computer language benchmarks game and optimize it so that your language is the fastest, we will allow you to do that. But at least we start quantifying it. So it's fastest or more centralized, according to what measure right. And finally, we're also doing is we're launching this new token, which is hopefully going to be the most decentralized by one measure, which is just sheer number of holders, because it can be gotten without capital you put in labor, and hopefully that'll give a much more equal Gini coefficient than than the alternative. Okay, so that's what we're doing. I'm open to ideas, you guys can email me if you have If you've got thoughts on this or or hate it or love it, what have you. And thank you very much.

Unknown Speaker  20:09  
anybody has any question? Question?

Unknown Speaker  20:13  
So have you looked at applying this model to some of the ancillary technologies like smart wallets? Because it seems like you went directly after centralization, obviously, of the Bitcoin client code, but what about attacks? Like what happened to Coinbase? Earlier this summer, which is really a fairly simple born vulnerability in the smart wallet implementation, which led to a lot of cost.

Balaji Srinivasan  20:36  
Oh, yeah. So So one thing I want to be absolutely clear about is I'm not going after Bitcoin Core, and I'm not going after aetherium, or anything like that. I'm just trying to propose a metric. And so the thing that you're talking about, I think, would be a, you know, measured by like looking at exchanges, right? If you had a gross concentration of trading volume in one exchange, that would be bad. Because if that exchange was hacked, or it had issues, or it was shut down by regulators, or what have you, then then we would would be in trouble. So um, you know, this is not singling out any one client. It's not singling out any one coin or anything like that. It's just propose a system for how we might start quantifying this thing. Question.

Unknown Speaker  21:19  
We're interested in talk and attract her question. How can you prevent cheat? If I know your metrics, as a developer, I can commit for from different accounts. As an owner, I can send my money to different accounts of exchanges cheated. We have a talk here today and so on. How can you prevent cheating in your metrics?

Balaji Srinivasan  21:41  
That's a great question. And I think the answer is going to be if you have a cheating and metrics, and people are at least thinking of them as important. And so I would argue that an imperfect metric, which is somewhat gamed, that we might have to modify and make more robust, is better than none at all, as an example, with Google's PageRank. Right? backlinks to a website are very useful metric for ranking, it does get gamed, but doesn't mean that the metric is not useful. You just have to adapt the system over time to deal with various gaming attacks.

Unknown Speaker  22:12  
Think one more question.

Unknown Speaker  22:18  
Yeah, so it seems like you're building a Metacritic almost for for exchanges and stuff like that. The question that I had was more around, what direct impact does decentralization have on a token? Ah,

Balaji Srinivasan  22:32  
yeah, so well, so the thing is that I think, here is, you know, my, my fundamental motivation, it's this, it is it's the optimization, you know, motivation, right. So for anybody here who's into math, or or, you know, engineering, right, you know, like, the theory of optimization has been absolutely critical for, like a huge amounts of machine learning over the last, you know, 1015 years. And so what I wanted to do is get a metric that the community could start working on, on and even if imperfect, we could modify it, we could, rebusify, etc. So that's a primary motivation. Regarding tokens, I think a good chunk of tokens, if successful, will probably eventually migrate to their own dedicated chains. Sort of like how you deploy on Heroku. And then if the scale gets enough, you'll move to AWS or even your own dedicated data center. I think, you know, folks will deploy to like an ERC 20 token at the scale becomes too much. You know, we'll see plasma and other kinds of things very promising. But it may turn out that you might want to move to a dedicated blockchain. So I kind of look at coins and tokens as being a little bit on a continuum, and we'll see what happens in the future.

Unknown Speaker  23:32  
Thank you. Thank you, Balaji.

Transcribed by https://otter.ai
