Unknown Speaker  0:00  
They're just trying to go live on top.

Unknown Speaker  0:13  
I tried to shoot.

Unknown Speaker  0:25  
I'm worried.

Unknown Speaker  0:28  
I'll give it I'll give it like a minute, but

Unknown Speaker  0:33  
I'm worried just channel.

Unknown Speaker  0:36  
So we're live right? How is that? I think

Unknown Speaker  0:41  
so this is streaming

Unknown Speaker  0:42  
live

Unknown Speaker  0:44  
channel. It's not on.

Unknown Speaker  0:47  
Yeah.

Unknown Speaker  0:48  
Yeah. So our job is to share this link

Unknown Speaker  0:50  
in the live

Unknown Speaker  0:55  
chat. The Silicon Valley a lot. Oh, yeah.

Unknown Speaker  1:11  
It looks like this.

Unknown Speaker  1:16  
Yeah.

Unknown Speaker  1:23  
So I think that

Unknown Speaker  1:27  
when we go to the meetup

Unknown Speaker  1:32  
seen

Unknown Speaker  1:37  
there otherwise, I mean,

Unknown Speaker  1:42  
I knew

Unknown Speaker  1:48  
what it looks like.

Unknown Speaker  1:52  
It looks like this.

Unknown Speaker  1:54  
took this trade argument.

Unknown Speaker  1:59  
Before I do want to share the link.

Unknown Speaker  2:08  
Like, five minutes.

Unknown Speaker  2:16  
We need this. We need this. So this is the live link up next. And then we're gonna

Unknown Speaker  2:30  
Are you know where I'm going to go down? The link

Unknown Speaker  2:46  
right there.

Unknown Speaker  2:50  
Yes, and this is the link. So that was

Unknown Speaker  2:56  
a lot because, well,

Unknown Speaker  3:11  
sorry about the way you're trying to get a live stream working and the tricky business we got YouTube going. So I'm Chris Steele. I'm the organizer of the Silicon Valley area meetup. We have a great event here for participants who I would love to have for all by themselves for like an hour or two hour long enough, but you know Wallah, we have all four of them together. Nice.

Unknown Speaker  3:45  
So

Unknown Speaker  3:47  
what are what are some important details? Yes. Great month, great speakers. Balaji Srinivasan Joseph Poon metallic Gudrun, Dan Benet

Unknown Speaker  4:00  
Dana will,

Unknown Speaker  4:03  
will kind of give us the speakers and manage the can run the panel at the end. But just to give you a bit of introduction to the meetup all get soft to give an introduction of myself. So we we have only one upcoming meetup. And that is in Fremont and have unusually

Unknown Speaker  4:31  
just a place to

Unknown Speaker  4:36  
you know, to to share the wealth a little bit. This is just an informal place. Informal meetup, coffee and crypto chat, you know, you know, discussing prices and all this stuff, you know, investing advice, which we never get at the meetup.

Unknown Speaker  4:52  
So

Unknown Speaker  4:53  
great well, so you might say what is the focus of the Silicon Valley theory meetup. You Nobody will discuss Icos every time and, you know, figure out whether they're worth it or not. Well, then my, my goal for the meat of this education, it's like data, whether it's developer education, or all kinds of centralized tech, I always ask, even if people are pitching a new token, I asked them, they make their presentation educational. Now they're trying to teach yourself, you know, it's not just not want to be their marketing.

Unknown Speaker  5:37  
So you might say, you know, some of you might

Unknown Speaker  5:41  
talk.

Unknown Speaker  5:44  
So here's the talk, I'd love to hear from you. And, you know, tell

Unknown Speaker  5:50  
me the

Unknown Speaker  5:51  
same. He's not here all the time. So here's your chance.

Unknown Speaker  5:54  
So

Unknown Speaker  5:58  
we'll, how decentralized will our decentralized future be all of the

Unknown Speaker  6:06  
consensus, the company be the new Google of the future? So you can try to answer that question, give me some real possibilities. You might talk about might give us a history of money. You've all Harare says money has a shared imagined reality? How does that interact with the

Unknown Speaker  6:26  
ether?

Unknown Speaker  6:29  
Give us a talk.

Unknown Speaker  6:33  
We're gonna have some.

Unknown Speaker  6:40  
There's all kinds of different scaling options, which some of which are going

Unknown Speaker  6:43  
to be talked about tonight. But you know, you could review those. Talk about proof of steak, you could talk about that elliptic curve, crypto and metropolis. We're always happy to have people give the tutorial talks on

Unknown Speaker  7:02  
solidity. It's, it's tricky for people to learn that.

Unknown Speaker  7:06  
So I guess, in general, I hope that this is acceptable to i

Unknown Speaker  7:12  
think i think this is something that vitalik is seized on to before but we're

Unknown Speaker  7:16  
always happy to hear

Unknown Speaker  7:19  
people outline the problems with aetherium, you know, that, you know, tell us ways in which is deficient, we're open minded, and we try to focus on you know, the problems of theory and not the problems of other platforms. So, Dan B'nai, please. Great. Thank you, Chris.

Unknown Speaker  7:41  
So, welcome to Stanford, everyone. I'm really excited to see everybody here. This is actually a special day on campus. I don't know if you guys know, but this is actually moving day. All the undergraduates are moving in and tonight is their first night, Home Home Alone, well in the dorms away from home. So there are lots of parents who are crying on campus is a very, very special day. Okay, so with that, we actually have a fantastic program lined up. So for three wonderful talks and under panel, the talks are going to be we're going to try to keep to 25 minutes, and then we'll do five minutes for q&a. And then we'll go and continue with with the panel discussion. And once we're done with the four main main talks, there'll be food outside, right in the foyer over there, and we can all hang out over and chat about aetherium over over excellent food. Okay, so with that, let's get started. So our first speaker is Balaji Srinivasan Balaji is the founder and CEO of 21. co. He's also a partner, partner, Dreesen, Horowitz. And I have to say I had the great pleasure of teaching our crypto class. Sorry, a cryptocurrency class with Balaji, who actually helped a great deal with the lab for the class. So Balaji looking forward to your talk.

Unknown Speaker  9:01  
Okay, does that work? All right, great. Okay, so, um, today, I'm gonna talk to you guys about is basically this idea of quantifying decentralization. So just by show of hands, which of you guys were at the blockstack? summit in July? Okay, only a smattering. Okay, so that's good. So some of this will be, you know, overlapping with that, but there's also a fair amount of new content. And so, you know, Dan was kind of intro me, that's me in a suit only picture ever taken. And so, basically, I'm actually a Stanford lifer. You know, almost 20 years ago, I was an undergrad moving into one of these dorms. And now I'm running 20 one.co. And we're going to talk about today are basically three things. So most of it is going to be about the motivation behind a simple metric for quantifying decentralization. And at the end of got funding for anybody who's a developer who wants to build a decentralized dashboard, we've got a great domain name at decentralized, calm. And so if anybody actually want to code that I can provide some funding. And you know, basically you can implement some of the ideas in here to make a real time dashboard across coins to look at these decentralization metrics. And third, we've got a new token. And it's not an Ico, you can just go and sign up for it at Central token calm. And basically the idea is, this also partially addresses some of the issues that I bring up in point one. Okay. So that further ado, basically, when we think about decentralization, this is something that everybody talks about endlessly. In fact, it's something where, you know, everyone's gonna say it's important, and in fact, is the most important property of blockchains, as opposed to traditional databases. But in order to say that something is more or less decentralized, you need to have a quantitative metric. So just establish that, you know, everyone agrees that decentralization is important. Here's satoshis launch of Bitcoin. Actually, it's like, about a month or so afterwards. But, you know, in the second sentence, he says, you know, the unique selling point of Bitcoin, the attribute is it's completely decentralized, right? Nick Sabo says, you know, it's what allows Bitcoin student army of computers vitalik, you know, was right here said, you know, it's viewed as a blockchains entire raison d'etre. But one of the words is perhaps to find the most poorly. And Adam Ludwin, who's a chain CEO says, with a very related thing on censorship, resistance, that destroy decentralized, astronomically better, they're arguably the only solution for certain kind of application. So everybody kind of agrees that this concept of decentralization is important, and that it's a key feature that distinguishes blockchains from centralized databases. And so the thing is that it'd be really useful to have even an imperfect measure with which to quantify decentralization, as I'll show later in the talk, imperfect measures of performance have a lot of improved programming languages, imperfect measures of what is a hot dog or not a hot dog have allowed us to develop apps, like in Silicon Valley, and in fact, the entire field of machine learning. You know, feature ization is an imperfect quantification of something. The thing is that if you can quantify something, even if imperfectly, once you can measure it, you can say, Okay, this improvement, or that improvement, increase the value of this function. And then you can actually plug that into an optimization algorithm, you know, many of you guys here, you know, if you're a Stanford students, you've taken convex optimization. But basically, optimization requires a so called loss function, something that actually measures the property of something. So if you want to design an optimally decentralized system, you need to actually have some kind of quantification of what decentralization is, okay. So this is kind of motivation, people think it's important. And if you could quantify it, unlike a whole body of mathematical tools that you could then apply to the situation, which would hold even if that qualification was imperfect. Okay, so to get there to get to a qualification, we're going to just introduce a few concepts. So first is this idea of the so called Lorenz curve. Now, this is something you've probably heard about, maybe from economics, the Rennes curve or the Gini coefficient. It's used by economists to measure inequality. And the idea is that inequality of money and centralization of power are actually similar sorts of ideas, right? So let's just review these two ideas, right. So with the Lorenz curve, basically, it goes from the concept of perfect equality, where everybody has exactly the same amount of something, let's say it's a theorem or a theory of mining capability, or, you know, commits to a codebase, if everybody was exactly equal, and they all had, you know, one bitcoin all, you know, 7 billion people, which is not actually possible, then you'd have a perfect equality like this, where it just it just basically be a straight line. Whereas if you had total inequality, only one person has everything, everybody else has nothing, okay. And so this is this concept of the Lorenz curve, which is basically this, this, you know, this graph. And the Gini coefficient is essentially a measure of how concentrated Lorenz curve is, it's zero when you're completely equal. And it's one when you're completely unequal. And these are kind of two examples along that continuum.

Unknown Speaker  13:41  
And so, so that's concept number one. Concept number two is that we can take a decentralized system, break it up into subsystems and measure llorens, and Gini coefficients for each one. Now, I want to be absolutely clear. The subsystems I'm about to show these are for illustrative purposes only, I don't argue that they are the only ways you can decompose these things. And the data is from July 2017, we'll do an update of this actually, I've got some funding for somebody wants to make a real time dashboard of it. So with those two caveats, let's let's take a look at these decentralized subsystems. The basic idea is that you can take a public blockchain and you can enumerate a set of essential subsystems. For example, again, just for illustrative purposes, let's say that we say there's the six essential subsystems, right? So mining, you know, the the, say, the distribution of block rewards across miners, the different clients, and you know, how they're distributed in production across, you know, nodes that, you know, you can actually see, you know, how, what percentage of shares owned by different code bases, developers and so within, let's say, the most popular repository Bitcoin Core get, what is the distribution of commits? exchanges, and you can see kind of what the distribution of volume is, is it all focused in one exchange or is it spread out nodes by country and then ownership by addresses, right? These are just examples of subsystems. And then for each of these, what we can do is we can calculate Lorenz curves and Gini coefficients, right. So you know, if we take the The last 24 hours of Bitcoin mining, it's actually fairly spread out. And a bunch of different folks have earned some. And so this actually has a relatively low Gini coefficient. It wasn't actually that that centralized at the time that we measured this. Client decentralization most Bitcoin clients are running Bitcoin Core, there's some that are running, you know, ABC, or, you know, Bitcoin Unlimited, or what have you, developers centralization. So by commits to core, basically, the idea is, you know, okay, how many commits to the first developer have, the second most commits? Are the consumer, the second most controlling developer and so on? That's actually fairly concentrated, not as much as clients can always also do this for exchanges and for nodes and for owners. So we can say, okay, you know, across exchanges, what is the distribution and share of, you know, trade for our daily volume, you know, across countries, how many nodes are in each country, the US and China and what have you, and across owners how much is held per address, given that we impose some threshold and it's actually important, given that most people in the world have zero BTC or zero aetherium, unless you impose a threshold, this is just going to trivially be point nine, nine, and I'll return to that point later. But given a threshold, you know, what does that owner decentralization look like? We can calculate these curves also for aetherium. Right? So you know, mining, decentralization a theorem again, just over a 24 hour period, client decentralization, Dev decentralization. So there's a lot of, you know, the top two people, you know, and get to actually contribute a very large percentage of the code. Exchange, decentralization, you know, like, again, volume across exchanges, no decentralization across countries and owner decentralization. Okay, so this gives some sense of given these, this choice of subsystems, this choice of metrics, how centralized they are. Um, but, and the idea here is that we can take a maximum over these insane, a decentralized system is only as decentralized as its least decentralized subsystem. And the reason for this, just to understand is there's like a decentralization bottleneck, right? If everything else is decentralized, but you have one component that centralized the system is centralized, because you can then go and grab that bottleneck. So as an example, if we just take a table like this, it doesn't matter in this particular circumstance that you know, Bitcoin is highly decentralized in terms of block word of the past point for hours, if you could just compromise a particular client on you know, then this is the most centralized row in that table. That is to say, you know, the number one client is like the most centralized, you know, component, again, by this measure.

Unknown Speaker  17:27  
And the other metrics wouldn't matter as much. Because if one portion is centralized, the whole thing is centralized. Now, one can object to the Gini coefficient, credentialing doesn't really give mechanistic insight into what you have to compromise. So we have a modification of it that we call the minimum Nakamoto coefficient. And the idea here is, uh, you know, basically to go back to the Lorenz curve, and say, Okay, what if we do something that's a little bit more intuitive? And we say, Okay, how many entities Do we need to compromise to get to 51%. So we call the Nakamoto coefficient, you know, motivated by the concept of the 51% attack in Bitcoin mining. And, of course, you know, in any given subsystem, you might need to compromise more or less than 51% to compromise that that subsystem. But the idea here is that we can, again, go back to each of these graphs, we can say, okay, you know, the Nakamoto provision is four, in order to compromise Bitcoin mining, you need to take your top four miners and compromise them to get 51%. But you only need to compromise one client in order to get more than 51% of nodes. But you have to compromise five devs to get more than 51% of commits, and so on, right. And 51% is an arbitrary number outside the context of mining, you could change that that threshold, but this starts to give a sense of how many entities you need to compromise in order to get past 51%. And we can do this across exchanges and nodes and owners. And we can also do this for aetherium, right? And we can say okay, you know, two miners in this 24 hour period, you know, mind most, no more than 51% of the theorem, and again, in this particular 24 hour period. And when we do all this, and we told it all up, we can look at these tables across Bitcoin and theorem and we can start to see, okay, you know, holders, at least by you know, this metric are actually relatively decentralized. But, you know, the client are actually fairly centralized in the sense that the most popular client has a very large market share across nodes. Now, all these things are subject to change over time, I'd like to get a real time dashboard together. So you know, if you measure this today, it may be slightly different and what have you. But the general concept here of using benchmarks to identify a decentralization bottleneck is I think, a useful and valuable one. Now, there's obvious objections, and I want to try to head some of those off at a pass, right? So first, the choice of subsystems matters a great deal. Second, you might not consider all sorts of systems important. And third, very important question is these kinds of measurements mean anything to the actually relate to concrete attacks or is just, you know, kind of fooling around. And so just kind of enumerating those, you know, to discuss those obvious objections. So for example, one point that vitalik brought up is If you measure get versus parody versus swarm, those are actually you know, especially get them parody or like, you know, independent codebase is completely independent code bases. Whereas if you look at the top Bitcoin clients, well, you know, Bitcoin unlimited and you know, ABC and so on are basically forks of the Bitcoin Core code base, they're not full cleanroom implementations like btcd, or what have you. It turns out the most popular clients in the Bitcoin ecosystem are forks, right. So you can argue that those aren't actually truly independent code bases. And because of that, then, you know, it's more likely there's going to be a shared bug across these, there's going to be a shared bug across given parity, right. Okay. So you want to define your subsystems carefully, and you can modify the metrics that I chose. That's why I said they're illustrative. They're not, you know, meant to be definitive, just to show the general idea. The second objection you might make is, you could say certain subsystems just don't matter to you. Right? So for example, Satoshi, you know, famously back in 2010, he said, You know, I don't believe a second compatible implementation of Bitcoin will ever be a good idea, right. So he would say, it doesn't matter, Dev, Dev centralization client, centralization, doesn't matter. But he does care about the 51%. Tech, at least at that time. And then third is, you know, subsystems, you'd want them to map to attacks. So basically, if you have 25%, compromise and mining, that might be more serious and 25% compromise accounts, because you've executed, for example, a selfish mining attack if that attack is feasible, right. And so when we just sort of respond to all of these, collectively is, if you enumerate hypothesize attack types, then you can choose your subsystems accordingly, and then measure the decentralization of each one by something like the Nakamoto coefficient. So um, you know, for example, let's say that you, you know, are looking at the news, and maybe, you know, country x might nationalize or even acquire mining companies, right. In that case, you'd want to measure the hash rate across countries and make sure your system was robust to an attempt to, you know, Chinese action nationalization of mining. Another example is, if you expect, the attack that you're looking at is there's a bug in one client's codebase, you'd want to have market share across nodes spread out, such that, you know, you know, if there's something like the remote crash bugs and be in Bitcoin Core, or the parity multi SIG bug, the entire ecosystem wouldn't be affected by one client. And just to be clear, everybody who's working on these projects, they're awesome engineers, and what have you, I'm saying nothing negative about them. Simply that you know, maybe not having all eggs in one basket may be a good idea, if you

Unknown Speaker  22:21  
if you suspect this might be an issue for the ecosystem. third example. So if anybody remembers in 1996, Phil Zimmerman was prosecuted for, you know, rolling out PGP. So if you know only one developer is doing all the commits for an important client, well, there's a history and there's a precedent for, you know, government, prosecution, or what have you of that individual. So having that spread out could be good if you think this is going to be attack and attack. For example, one that's not even theoretical. So you know, country show sound exchanges. So in this case, you know, like the Chinese showdown of btcc, you know, kick coin, and we'll be, you'd want to have volume measured across countries and make sure no one country has, you know, too much of the volume such that if the country shuts it down, you can still operate in other countries, right. Fifth example. So if you've got nodes that are running on AWS, or Google Cloud, and they suddenly decide those nodes violate your acceptable use policy, you don't want to crash the network, right? So you want to make sure that you've got unique server entities that are that are serving them. And in some cases, for example, people will dial up hundreds of nodes on, you know, AWS, or Google Cloud, that doesn't really fully add to the node count in the same way, it might be able to process traffic, but it's not independent control, because it could all get shut down at the same time. So actually, having independent IPS indepen control is very important, if you believe this attack is a potential one. And then last, just as an example, if you have a very, very unequal distribution of digital currency, at some point, there may be a political attack, where those without digital currency vote or support seizure from those who have it. So you know, many examples of this in history, but a very recent one is in Venezuela, you know, people, you know, voted to reach Wheatland, and that really backfired. NPR, you know, has lots of articles on this, everybody does at this point. So that's an example of how if you believe this is a potential attack in the future, you'd want to actually, you know, maximize decentralization in terms of ownership across individuals. And again, these are not you know, just to show you some screenshots. If you've got mining centralization, then China banning Bitcoin executives from leaving the country, that's a feasible attack. If you've got client centralization, then you know, the Bitcoin Core or the BU or the parody multi SIG, you know, tax would be bad. If you've got dev centralization, like this Zimmerman, you know, prosecution of Phil Zimmerman, then, you know, you don't want to have only one dev who has a knowledge, modify the codebase exchange centralization, you know, if it was all centralized in China, we would be in big trouble. Fortunately, it's not there's Japan and there's, you know, the US and others. Um, you know, this guy got his account to shut down randomly by Google. So you really would not want Google or Amazon to be able to flip a switch and be able to turn off all nodes. And, you know, as an example of how holder centralization could be trouble You know, the IRS actually impose a threshold and set anybody over $20,000, they want to investigate if they've done damage to a business on Coinbase. They're trying to just get a list of everybody, right. And so if that list was millions and millions of people, then there'd be more of a political outcry against, you know, this this sort of thing. So, essentially, this is how you can think about decentralization providing resistance. Different kinds of attacks, lead to different subsystems and measures. And for each of them, you can calculate a like a Nakamoto coefficient, you can say, okay, in the event this attack happens, what is the impact on the Nakamoto coefficient? How, how much more centralized does it become if that attack happens? And how can we allocate resources to prevent that from happening? So, two more points, and then I'll quickly finish up. One thing I want to, you know, talk about is that even imperfect quantification can lead to useful results. I don't argue that this is the final metric or what have you. But if you guys saw Silicon Valley, you know, the show, hot dog or not hot dog is actually a very sophisticated machine learning application, you wouldn't believe it, right. And, you know, so there's a great medium post on how they designed it. But one of the things just to keep in mind, if you guys have ever done any machine learning is no one feature determines what a hot dog is, you know, when you're doing supervised learning, or anything like that, it's all these imperfect kinds of heuristics. But collectively, they start to bring this fuzzy kind of object, this notion into being a combination can give you some results. Similarly, if you've seen this benchmarks game, this is actually, you know, a, it's very carefully phrased, this website doesn't say which programming language is the fastest, just like we don't say which blockchain is the most decentralized. It says which programming language is the fastest on this toy benchmark. And in the same way, which blockchain is most decentralized on this benchmark, that's kind of the thing I'd like to drive towards where we acknowledge benchmarks are flawed, but a collection of benchmarks can potentially give useful results and help us think about things. Finally, I would say, you know, decentralization cannot be in my view achieved through one coin alone, every group of you know, coin developers, token developers has a different thesis on what kinds of attacks they consider reasonable or probable. And so you know, strength and numbers in a lack of monoculture, I think, would be good. So that's one of the good things about this current Ico and token and coin mania is we're getting many, many different kinds of approaches, each of which hopefully, is robust in different ways, to different types of attacks, and decentralization.

Unknown Speaker  27:21  
So just a summary, summarize, sort of propose a simple metric of decentralization. It tracks with our intuitive notions, given a list of subsystems, you can start with attacks, get subsystems that correspond to those tasks, and then measure the Nakamoto coefficient for each one. And once you've got this, you can start putting it in dashboards and objective functions. So just in terms of what's next, we're doing two things. So we, if any of you guys, you know, want to come up afterwards, we'd be happy to fund a dev to do something at decentralized comm that URL doesn't resolve right now, but we have it. And so we'd like to get something like this benchmarks game, with, you know, Rennes, and Jeannie and Nakamoto coefficients for many different coins and subsystems. And each person just like the benchmarks game can check off the metrics that they think of as the most important, and they can use that to argue with somebody on Twitter or Reddit, right, which is what we all love to do. And, you know, so So this, I think, will be fun, because it's something which at least starts to quantify this thing that all of us know is important. And I'm not saying this is the final way to doing it. But I do think that starts to move us in the right direction. The second thing we're doing is if you go to social token.com, we do have a new kind of token, it's not an Ico. In fact, part of the goal with this is that rather than people giving us capital for tokens, they would give us labor for tokens, and many, many more people, can you go to a website and click some buttons then can afford to invest thousands of dollars in a new coin. So if this works, you know, it would be highly decentralized by at least one measure, which is the number of holders and that would protect us against a political attack where too many people don't have digital currency, and they get mad at the people who do. I don't argue that it protects us against a mining attack. I don't argue that it protects us against a node attack or what have you. But I do think that a multitude of different approaches to protecting against attacks and decentralization is valuable. And this hopefully can contribute to that. So you can check that out at social token comm if you want and just go and go and sign up. So um, that's it, you know, thank you, and happy to take questions when, you know, for a few minutes, and then maybe we do the panel afterwards. Yeah.

Unknown Speaker  29:25  
That's all we have time for a few questions. Joseph, do you want to come up here and get set up? Yes, please.

Unknown Speaker  29:38  
But that doesn't necessarily seem like

Unknown Speaker  29:46  
yeah, so that's a bell. Do you want to repeat the question?

Unknown Speaker  29:48  
Yeah. So he said anything that has a really long tail is going to have a high Gini coefficient, but it doesn't mean that it's not decentralized. I would agree with that. And that's actually why we think this Nakamoto coefficient is starts to get to a better idea of what it is Because basically the knocking on coefficient is more intuitive, you can say it's a number of entities, you need to compromise to control 51% of that subsystem. That's like something you can get your head around. It's like, okay, we need to control for minors in order to control 51% of hash rate, or we need to control 72 people in order to have more than 50% of aetherium. Right, that kind of thing. And and that's, I think, easier to reason about.

Unknown Speaker  30:22  
Yes, sir. Let's take one from the other side.

Unknown Speaker  30:24  
Hey, thank you for doing this research is super important.

Unknown Speaker  30:27  
I've seen a similar kind of analysis in brain research, and it might be useful to look into, if you crack open dead people's heads and look at their neural topology.

Unknown Speaker  30:36  
Turns out that people are in a

Unknown Speaker  30:38  
bell curve neurotypicals have average many column lengths. And then if you have unusually short, like hyper connected clusters in your brain, then you're more autistic. And then if you have like, more stronger Global Connections, but weaker local connections, then you're more dyslexic. And, and then so they take these things like the poor metric, you know, between different points in the brain, and then they have a threshold train, like you have that they move the threshold from zero to one, and then record how many of the connections are sort of the paths are less than that, that threshold or more, and how does that grow? And so if you get, like really convex or concave anyway, I

Unknown Speaker  31:15  
just want to say I thought it'd be

Unknown Speaker  31:17  
cool cryptocurrency search,

Unknown Speaker  31:18  
if anybody wants to really make a sacrifice for aetherium. I'm happy to crack it open and take a look. No, but actually, awesome idea. Thank you. Thank you. Yeah.

Unknown Speaker  31:45  
Well, I think we're actually, you know, on the cusp of something, which is a step in which we can organize humans, right, like so, you know, aetherium, you know, is not something that vitalik is actually CEO of, right? Like, it's something that is a decentralized organization where anybody can enter and take, like almost equivalent of an equity stake, and then work to make it better. And there's no direct command and control sort of hierarchy. And yet, folks have made money on it a lot of a lot of money. Um, so, you know, that preserves some of the aspects of capitalism in the sense of folks in collaborate to, you know, put food on the table, but it is not as command and control. So, you know, I'd say economics is going to take a veto over the next few years. Yeah.

Unknown Speaker  32:26  
One more can we take one more question? Yeah, please.

Unknown Speaker  32:34  
So my question really is around, we've talked about decentralization. And you introduced a social token, for example, which is really introducing it to a broader amount of people. But is there is there actually an aspect of the self selection that would actually ends up with the steel being centralized around those who would have access or know about our participant participate in it in the first place? And what's the impact of that initial connection between those aspects?

Unknown Speaker  32:59  
That's a good, good question. So in general, I think lots of technologies, start with the 1%. And then they get to the 10%, and then the middle class, and then they get to, you know, the nine 9%, and so on. So cell phones are great example, in the 80s. That was just a province of super rich guys on Wall Street. And you know, today, we've got billions of cell phones and you know, folks in India and Africa and so on, actually, I saw a stat recently that more than 50% of world now has a cell phone. And so the thinking is, this is I'm not saying it's a final step. But this is the next step, I think in starting to get a much larger cross section of folks involved in digital currency. And then you know, from that, hopefully, we can build from that base and get it even more mainstream over time.

Unknown Speaker  33:40  
It's very cool. Thank you. Fantastic.

Unknown Speaker  33:48  
All right. Our next speaker is Joseph Poon. Joseph. Of course. Joseph, of course, is one of the CO inventors of the lightning network, and is also running a new blockchain, which is going to tell us about all yours.

Unknown Speaker  34:01  
Thanks, Dan.

Unknown Speaker  34:05  
So as a show of hands, how many people here yesterday, so I don't want to be too redundant. Oh, great. Not that many. So I can just repeat the same slides. I'm putting some new stuff but okay. So how many people have a general idea of how it works or like read the paper descriptions?

Unknown Speaker  34:26  
Okay, about 30%. Okay, cool. So, um,

Unknown Speaker  34:31  
I think as a background, I think it's important to sort of

Unknown Speaker  34:35  
understand and reason about the motivations behind plasma. So as we all know, the security of blockchains are derived from everybody validating what you see on the blockchain and not trusting what other people tell you. So your code verifies what you what you what you receive the blocks you receive and the information Receive to ensure that, you know, all the data is correct because there's no single trusted authority of the correctness. And that essentially creates upper bounds in the amount of computation that you can do. Because logically, if you need to verify, you know, for example, all the financial transactions in the world on chain, that creates a lot of expense. And this is especially true and magnified for computation. If we want to be able to compute complex contracts, then everyone else would hypothetically have to compute these complex contracts. So there's a lot of emerging work with channels and other technologies to be able to reduce the amount of computation necessary to be able to do that. And if the blockchain is sort of the ground truth of the world, then we should minimize the amount of truth being asserted to it. And you really don't want to disagree about this ground truth. Because if you disagree about this ground truth, then, you know, the system sort of breaks down, it's sort of like if you had sort of like two Supreme Courts that disagreed with each other, and you sort of didn't know which was, which was which, which, which to build case law upon, then, you know, you can't really build some some system, which we can be able to reason about and build contracts around. And on a technical level, that means that state synchronization across the participants becomes imperative, you have to enforce the current state. With global data, it's easy, because everybody has access to the current state, you know, everyone receives the blocks, evidence, analyzes and processes the same blocks. And therefore you all have the same set of data. When you don't have global data, then that issue becomes a little bit more complicated and is going to be sort of the central theme of a lot of the work around scalability. It's sort of the you know, the other side of the coin, when it comes to reasoning about scalability. And, in this case, withdraws or withdraws is sort of a misnomer, but it's sort of when you're moving funds or moving the state between the different chains, that becomes essentially you're saying, okay, we have this global state, we're pre committing some amount of funds to it. And this is true for any channel type construction, you're pre committing funds to it. And when you pre commit funds inside you, you have local agreement between two participants on who has what within that fund, right. So if you had one eath, and you're splitting it up, point 6.4, between you and you and some other person and then updated 2.5, point five. In order to move forward, you both need to have sufficient data in this case, the data is the other person signing off on on the new state. If you don't have access to the data, then you can't prove the current state. And that becomes a huge problem. And in this case, it's resolved just by forcing hard synchronicity between two parties. Well, harder over time. eventual synchronicity between two participants whereby when you move to the next stage, both parties have both signatures, each other's signatures. And at that point, you sort of, okay, you have you have the data available. Using this construction becomes increasingly complex, forcing synchronicity across all participants. Because when you have complex contracts, that means that you're going to have a lot of participants who may not be online at the same time, and may not be able to compute all of the data at the exact same time. This creates constraints around you know, likeness whereby you know, likenesses, basically, everybody has to be online. And it creates incentives towards halting the system. So if you, if you have, you know, some computation between 10 parties, then if like three of the parties stopped computing or signing off on things, then they can sort of help the system very easily. So you sort of faced this, like, you sort of face this challenge whereby you choose either,

Unknown Speaker  39:20  
you know, greater robustness around keeping the system operational versus security. Additionally, you want to minimize the amount of ledger entries on the chain. But really the data availability problem permeates all of this. So, what is plasma? I'm going to run through this a little bit shorter compared to the presentation yesterday, you can watch it online if you want to see a little bit more in depth, but I want to give more time towards reasoning about the economic models and we sort of use cases for this. So plasma is the construction of block chains and block chains. So you have this, you have some block chain and then Have some which spawns a child blockchain, which can spawn further child block chains. And that's sort of how you how you reach scalability. And it's functionally a set of smart contracts that runs on the blockchain, above it. Or in the root blockchain, actually, which everything else derives from its security from. And this smart contract effectively defines the consensus rules of the child blockchain as the form of fraud proofs. So it's not actually you're not actually directly running the blockchain, on the on the blockchain, in this case, aetherium. In your only actually computing things in the event of disagreements about matching consensus, so if you violate the consensus rules, a proof is given on the root chain, and that block gets rolled back. And we'll go into that in a second. And by constructing things in a set of nested chains, in order to do computation across these nested chains, you have to reason about this in a different way, because you're dealing with many different systems. So the standard way to deal with disparate systems is to use something called MapReduce. So MapReduce is functionally when you split up the work, you have a map function, and when you combine the work, there's a reduced function. So there's, that's a way to distribute work and sort of understand the result from many different computers. Plasma is compatible with on chain scaling solutions, such as sharding, various block size constructions. So essentially, if you have better capacity on the root chain, that should be able to increase the robustness of plasma as well. So if you increase the root chain gives you you know, some X Factor, plasma can run on that and give you a further you know, y factor. And above that you can run channel construction such as lightning. So when it comes to the design goals, the design goals of plasma is to show that it's possible that blockchain, the blockchain can encompass a huge part part of the world's global transactions, not just financial transactions, we're also talking about computation in general, it's, I think, it's really going to be possible to put a lot of, you know, data storage, computation, web services, all of that on the blockchain. And it's, I think it's technically feasible to do so even with the technology we have today. We just need to write the code and then it'll get done. Post metropolis. No further changes are necessary on Ethereum. And this can get you you know, payment and ledger scalability, ledger, scalability is an interesting one. Because, you know, if if vitalik says, you know, a blockchain wanted, one of the key components of a blockchain is, you know, the data storage component, and then you have computation on that storage, well, you don't want to have too much data storage on the blockchain, otherwise, it becomes a little bit cumbersome for anyone doing full computation. So this can give you scalability on, you know, the state as well. So what is plasma really look like? It's really just a set of libraries, and you add on some business logic. So this business logic can really be anything you can by default, let's say you have a payments chain is like the default. And then you wanted to add on some social network, like crud components to it. And you can do so and suddenly now it's, it's, it's a social network chain, you just write the code, upload the smart contract onto aetherium. And then now you have some social network. And as long as you design the economic incentives correctly, it'll perpetuate and exist. And the operators of that chain are economically incentivized to run it. You can run create, you know, decentralized applications, such as a decentralized exchange, or you can run you know, your own private blockchain running on top of a public network.

Unknown Speaker  44:16  
So

Unknown Speaker  44:18  
this construction allows you to create applications, which you and a set of participants are able to verify and enforce on the route chain. If you're not, if you don't care about the results, if you don't care about the computation, let's say you're not using the social network decentralized application, you don't really need to look at it at all, you're not really computing much at all, you're only computing very, very small commitments on the root chain. And that creates great efficiency for not only people using the system, but also you know everyone else on the network because they sort of don't need to pay attention and didn't really look at it at all.

Unknown Speaker  44:56  
That's just a bigger picture.

Unknown Speaker  44:59  
So What type of data is committed on the root chain? in most circumstances in nearly every circumstance? In this case? Do I have access to a mouse? Oh, fantastic. I do. Can you guys see that? Oh, great. Um, so when you have periodic commitments to the root chain, when you create a new block in the child block in this plasma block number one, there's some transactions that occur inside, there's some state inside, and they commit a hash of that block. And it's signed by the block creator. And it gets submitted to the root chain, and the route chain and everyone else running Ethereum doesn't really do anything with it, you just like the data stored, it's okay, whatever. Because you're not actually the data isn't actually stored on the route chain, when it comes to, you know, let's say there's one eath in this plasma block, you know, like, it's just one eath is just stored in the contract. There's no accounting of like, who actually owns it, it's counted inside this inside the plasma block itself. And it's only when it's either withdrawn or disputed, that there's enforcement of correctness. And in this case, enforcement of correctness is the most important when it comes to withdraws. Because and this is a bigger picture of that. So essentially, let me backtrack a little bit. Alice, when she she holds funds inside plasma, this plasma block, and the only thing being submitted is, let's say 100 bytes, or 200 or so. And this in this block can possibly have like, millions of transactions in a single block, and ideally, not millions, let's say like, hundreds of thousands, but you can get net millions. And you only submit, you know, 100 bytes, and it's enforceable. And we can go We'll see how later how it's enforceable. So here's the issue, right? If only a small amount of data is being submitted to the root chain, what happens when there's an invalid state transition? After all, Alice, you know, the, the creator of the next block could say, well, you know, it's not Alice's money. Many more, it's Mallory's, and I'm friends with Mallory. So I want the money to and Mallory's giving me a cut. So because this data isn't on the route chain, what happens then, right? Well, in this case, if you're Alice, and you receive, you have blocked data number four available, block number four data available, what happens is, is that you can submit a proof on the root chain, because a smart contract in plasma is going to say, but it's going to allow you to submit proof that block number four is invalid. And as a result, block number four gets rolled back. So this is sort of what happens there. Before, it looked like there was going to be block number four. But anyone that has observed block number four can submit a proof on the route chain, which includes, you know, Merkel proof that, you know, this was included in block number four, and then perhaps the transaction itself, which, you know, let's say didn't have a correct signature. And then and then everyone on the Ethereum network processes that and says, Oh, yeah, this didn't have a correct signature. And the smart contract said, it must have a correct signature. So then what happens is block number four gets rolled back, the creator of block number four gets heavily penalized. So because of this penalty, you're going to be economically disincentivized, from propagating invalid blocks.

Unknown Speaker  48:54  
But here's the problem. Let's say, and here's the fundamental problem around, you know, when you're dealing with interactions between multiple blockchains and want to move funds between multiple block chains, is that in order to prove fraud, you must have access to the data. Remember earlier, I was talking about how data availability was sort of the fundamental problem that we're thinking about. And the reason for that is because with fraud proofs, you need this block data and what one what the attacker can easily do is submit the block hash sign it and not tell anyone and after a while, they can attempt to withdraw your money, because you know the withdrawal proofs will look legitimate and no one else will be able to prove fraud. So, what you do is you pre construct the design, so that if if you are not able to receive the block, after any blocks after some set of time, you can withdraw some withdraw your funds. So the interesting thing is your You can't actually prove on chain, whether data is withheld or not easily. You know, it's sort of like a he said, she said situation, because it could be a situation where it's like, okay, you know, alysus, like, Well, I didn't get the block data. And let's say Bob is the block creator. And Bob says, Well, I gave Alice the block, you know, like, there's no way to really prove that via very, very expensive interactive process. And which basically requires dumping the block of a blockchain. So you that's sort of always been this, like, weird wrinkle in designing these systems. So what plasma does, and the solution, in my view for these types of situations, is you pre design it so that you can exit after some, during some like time window, where during this time window, if you haven't spent the money, and it creates this challenge game, if you haven't spent the money, you're going to be able to access it. And by broadcasting a transaction on its parent. So this sort of the bigger picture, and the idea is that, let's say Alice only has access to block number three, she doesn't have access to block number four is suspicious and concerned that there's a state transition here, where you know, let's say Alice sends her money to someone else. And she doesn't have access to this block data, and therefore, it's unable to prove fraud, she submits an exit on the route chain, whereby she says I'm going to exit from this blockchain and has to wait some time period in order for that exit to be fulfilled. However, that requires Alice to be able to have space to submit the block or submit the transaction onto the chain. So in order to make that economically efficient, what we could do is constructed instead, as a nested set of trees, with possibly disparate sets of participants. So it's possible whereby, you know, there's this first tree depth, this is Bob, and this is run by Carol, and this is run by Dave, if you're Alice, and any one of these fails, you just go to the other ones in order to, you know, juggle your position around. And in this case, this is sort of the example of that, whereby if this one fails, the one in red, you broadcast on the first tree depth, and then you move on to the second tree depth. And that way, the transaction doesn't get broadcast on the route chain. Of course, if all of these three fail, you thereby do need to broadcast on the route chain. And you know, provided that there is block availability. The fun part about this is that if you hold funds on, let's say, this third tree depth and blue, you only need to watch its parents, because its parents are the ones enforcing your state. And, you know, if, let's say the second tree depth, you know, some submit some invalid state transition, you're able to prove fraud on its parents. But essentially, if the other ones in grade don't affect you directly, you don't need to watch the chain. So this can create significant efficiencies when it comes to scaling up on the validation and light client side. So if you're a client, you can do possibly full node validation of the first second and third tree depth, because you're not actually having that much that many state transitions, you could imagine this nesting out to thousands of different blockchains. And you only need to worry about the state transitions, perhaps which is happening, you know, maybe there's only hundreds of transactions occurring per day. And there's only commitments occurring periodically to its parents. So as a result, you're probably only processing data and perhaps the 10s of megabytes in order to it. Whereas the entire network itself is processing, you know, many, many, many gigabytes or terabytes of data.

Unknown Speaker  53:52  
Because we are constructing things in chains, like I said earlier, you may want to be able to use be able to do computation between these states. And MapReduce is, you know, a pretty standard construction. There's a lot of literature on it. There's a lot of literature talking about how to do different things like sorting, like various, you know, basic computing algorithms, whereby you can just use that and plug it in, plug the algorithms in and use it to be able to reach scale on blockchains. the only the only difference in this case is that we are Merkel icing the commitments of the states before and after of the state transition in order to make proofs of the state transitions. So it's basically provable MapReduce in a certain way. That way, compact provable that way when you send work to a child chain, you know in a map function, you are committing to the data being sent to those child chain and then when you do the reduce function, you are also committing to the end state of that to the parent chain. This allows you to reach scalability and be able to reason about how to delegate work across these different block chains. So when it comes to future work, there's future work on mechanism improvements. So creating more incentive for operators to incentivize correct exits, in minimizing block withholding attack incentives, so more mechanisms around that. Right now, the mechanisms are fairly robust so that if you're you, as an individual are able to get your money out provided that there's block space. However, you want to minimize the incentive for the operator to attack the system, even if you as an individual are able to get your money out. And one way to enforce this as if recursive snarks or Starks exists, then then basically you just pump the problem away. But ideally, I strongly believe in defense in depth. And ideally, you want the system working robustly with just an open system with like, very simple math. So you know, one can reason about the sort of, in the long term, how people reason about blockchain security. You know, the first line of defense could end up being hardware security modules. Whereby, you know, you just say, Okay, if you're a block node operator, you have some HSM. But I don't want to rely on hsms. Because, you know, I strongly believe in open source, and they kind of suck, because they sort of create centralizing effects when it comes into sort of like, because there's strong network effects around that. The second line of defense could be something like z, k, snarks, or Starks and then the third line of defense could just be an open, open mechanism game between between the blockchain whereby you can submit data in time for disputes and stuff like that, which is sort of the core design of plasma. Ideally, you have the system work with just that component. And the other ones are just a convenience. So additionally, there's some additional research necessary for token incentives, especially token incentives between the child chains. And, you know, additional aspects related to you know, hybrid proof of authority proof of stake constructions. So what does this get us I believe, I think like, it's, it's interesting, because I think it creates a way to reason about the the sort of future blockchain scalability and it's sort of rhymes with the way our society works. Today, we sort of have a court system. And if you view the blockchain as an adjudication layer, the highest mode child chain is sort of like the district court or the local court. And then its parents are sort of like the Court of Appeals whereby if, you know, your district court messes up, you can sort of like go above to its parents. And if the decision if the court of appeals messes up, well, you go to the Supreme Court, in this case, aetherium, or the root blockchain is the supreme court, it's, it's sort of a source of ground truth. And that's sort of where you submit, you know, your smart contracts, that's where, you know, everything is sort of like that's, that's the true consensus layer. But you don't want to clog up the true consensus layer, you don't want every court case going to the Supreme Court, that doesn't really work. You know, it's just a couple old guys, you know, you don't want to you don't want to like they're old enough, already, you don't want them doing too much work. So that's sort of, you know, like taking that analogy, you don't want everyone in the world computing it, you know, like, my computer doesn't want to do that, right? It's a little bit old, too. Oh, that means I'm going over time. Okay, let's just run through this, you already read it. So, um,

Unknown Speaker  58:46  
what I really want to talk about is like, what this means when it comes to proof of stake economic incentives on the second layer. In this case, we're talking about Bob creating all the blocks, right. But you can also do something where a lot of people create the blocks or a set of participants create the blocks, you know, instead of Bob, it could be Bob Carol, Dave creates the blocks and they each take turns, or they can do you know, some type of majority construction, you know, there's a lot of different like, there's a lot of different distributed systems literature on how to split up the work. The fun part is because this is a second layer, you can sort of punt on the problem because things like creating timestamping you just like okay, the parent chain does the route chain does timestamping a theorem does Time Stamping so you sort of like pump that problem away. So actually proof of stake on the system becomes really really easy in comparison, but on the other hand, the constructions you design on top of here may not be applicable for you know, proof of stake on the route chain, which the theorem foundation is doing like amazing research on

Unknown Speaker  59:50  
so

Unknown Speaker  59:52  
what's interesting is is that you can this is fully compatible with a lot of you know, new models new economic models when it comes to tokenization. Currently, a lot of people talk about use tokens, you know, like you, you buy a token in order to, you know, use some functional, functional thing on the blockchain. This is more closely aligned with proof of stake tokens. So, it's similar to like, you know, using some type of token on some root chain, you know, eth, or something like that, eventually, if you can do eth, bond or eth and do proof of stake on that, then you can get some return for holding eth. And then the to reason about it with, you know, the gas stuff, it makes a lot of sense. So, what you could do is, you could be paying eth fees for the transactions on the route chain, and the disputes on the route chain, but the transactions going on inside this plasma chain. The person that gets to create the blocks are the token holders. So what happens is that it's a proof of stake of the token holders. So you issue some ERC 20. And by issuing some ERC 20, the owners of that have a right on a proportional share, to be able to generate blocks, and the person that generate blocks, of course collects transaction fees. So there's an economic incentive to be able to, you know, collect transaction fees and to run the blocks. And what's interesting is, is that, you know, you have a situation, which is not the similar dealings of the principal agent problem with that we have with securities, right, with the securities, you're delegating trust, to another party to act on your behalf. And because you're delegating, trust, you know, you, you sort of, you know, like, you sort of have to trust them to be able to operate correctly, you know, will the CEO, you know, operate my company, and not just like, take all the money, right? In this case, you are directly taking action on the system. So, by having a specific stake, you know, with some, some, let's say you own like 5% of the coins, you can create 5% of the blocks or on some or collect 5% of the fees. If you take action, which is incorrect, you get penalized. If you take action, which is correct, or your you know, transaction selection algorithm is better than another, you can collect more fees. So it's a different construction than what we deal with today in society. But you know, by owning perhaps a portion of you know, the tokens, you can construct a model, whereby you're creating a decentralized network that nobody really has full control over, right? Imagine if you created a social network on top of this, you've created, you know, let's say, a Reddit clone or something like that, you write the smart contract code, uploaded onto aetherium, and then issue some tokens. And it's the token holders and assume the tokens are widely distributed among among society. Well, it's the token holders creating blocks, and they're gonna keep creating blocks, right, because like, they're economically incentivized to collect transaction fees. Who really owns the system at this point, right? It's a lot. It's very similar to like, who really owns aetherium, who really owns Bitcoin? Well, I mean, I guess it's the token holders, but like, you know, the guy who like uploaded the smart contract code, he can't like, call he if he said, like, yo, guys, let's let's let's shut this down. The token holder is gonna be like, no, right? Like, if metallic was like, hey, Ethereum is done, like, how about we just like, close up? Everyone's gonna be like, No, no, no, we're keeping this running. Right? So like, it's going to be a different dynamic that that could exist. And as a result, you're sort of removing the trust and trusted institutions to be able to operate on your behalf. And this is sort of like a new model that can exist on top of aetherium. That is that has significant scalability. And there's some use cases.

Unknown Speaker  1:03:56  
Sorry for running over time.

Unknown Speaker  1:03:58  
Why don't we take one question while Vitaly gets set up? Yes, please.

Unknown Speaker  1:04:17  
This is completely unrelated. So the question is, how does this Oh, sorry. Yeah, the question is, how does this relate to other consensus algorithms? This is not inherently a consensus algorithm. In that sense. You can run whatever consensus algorithm you want in terms of the proof of stake mechanism. You can run a proof of authority where it's just one guy signing. The key design here is that designing around exits under a child chain construction, and how to reason about with block space availability when it comes to data availability, when it comes to ability to enforce fraud proofs when it comes to being able to scale up from You know, from, you know, ledger, the ledger containing the the memory state, and to be able to abstract that away into specific applications, and to be able to create decentralized applications to whereby, you know, we're talking about potentially billions of transactions per second, with incredibly, incredibly low costs. Okay,

Unknown Speaker  1:05:19  
great. Thanks.

Unknown Speaker  1:05:27  
All right. Our last talk before we get to the panel is by metallic, who's going to tell us about the future of aetherium.

Unknown Speaker  1:05:32  
Great. Okay, so today, I'm going to also try my best to make this sub shorter than what I did what I did yesterday, yeah, in San Francisco. But I'm going to talk about basically or blockchain scaling, and try to give a kind of more precise understanding of basically what is it that makes scaling blockchain scaling blockchains, and specifically scaling blockchains, to the point where they process more transactions than what one single computer can process a very difficult problem. And particularly later on, I'm going to talk about something called the data availability problem, which turns out to be one of the main challenges in doing this sort of thing. So to start off, just to introduce a bit of notation, this is called a blockchain. It contains blocks, every block contains transactions, those arrows represent every block containing the hash of the previous block, everyone's still falling. Now, I'm going to represent I'm going to use this number c to represents the computational capacity of a node. So C is basically the quantity of the number of transactions that a node can process we will say per second, we can say per block interval, either in terms of computation, or in terms of storage requirements, or in terms of bandwidth, we just assume that all of these are proportional on roughly equivalent to each other. And in a simple chain. And we'll also say that the transaction capacity of the network is also going to have to be ofc. And the reason for this is simple. Because if the transaction capacity of a network is greater than c, or the value of C, then that means that nodes can't process all the transactions, nobody can run a full node. And so the watching doesn't work anymore. Right? So there's a few incomplete, so a ways to scale a blockchain. So all of these are technically scaling solutions. But they all come with serious drawbacks. So here's the first one super big blocks. And so increasing block sizes definitely does have its place. And you know, there are, I think there's definitely a lot of cases where you can get quite a lot just by increasing a few constants up. But if this is your only scaling strategy, and you try to push it to the limit, then you have a problem. So the problem basically, is that if we increase this from C to 1000, times C, then Okay, the network capacity goes up from C to 1000 times C, you've increased your transactions per second by a factor of 1000. But the load on each validator also goes up by a factor of 1000. So the amount of the amount of transactions that each node has to process only goes up by a factor of 1000. But, you know, we've made an assumption already that this is larger than the millions of transactions that are known that a regular computer can process and so your only notes are going to be inside of data centers. So this is strategy number one. This is strategy number two. And this is kind of the strategy that the crypto space has already been implicitly going towards whether intention intentionally or not, right? So big one transaction fee is in many cases over $1 and some sometimes over $5. Well, okay, fine, use Bitcoin cash, while Bitcoin cash transaction fees go higher, while use aetherium. While aetherium transaction fees go higher use aetherium Classic Ethereum classic transaction fees go higher, we'll use DOS, dos transactions are still cheap, right? So it basically just like keep it going. And, you know, if we get to a, quote, mainstream adoption, you know, we'll be going all the way down the list until we get to, you know, making transactions and coins with names like Trump coin and project decorum.

Unknown Speaker  1:09:30  
So, this is technically a scaling solution, right. And this gives you a network capacity of 1000 times See, the validator load is only see because you know, it's every every user only really needs to be a node on one blockchain. And you know that each individual blockchain is stolen, we're going to have ofc capacity. The problem is, each and every one of these chains is going to have 1000 times lower security. So you know, if you look at Trump coin The market cap is something like $600,000. Well, probably any single one of you could basically if you really wanted to afford the money to personally run a 51% attack on their blockchain. So you know, this is like the. So this is the second approach. Now, merge mining is another approach. And this basically says we have 1000 old coins, but we let people mine many old coins at the same time. The problem is, this also doesn't solve the problem. And the reason is that, you know, you have the main parameter that you can adjust here is the, like, on average, how many old coins does each miner end up mining, if the number is close to one, then you're not actually gaining much security, each individual coin is still, you know, 1000 times easier to attack than it was before. But if you're going if the number is very high, so if the number is close to all of them, then basically miners still have to process every transaction. And so you've actually done that. And basically the exact same thing as this, except technically, it's split up into 1000 bucks. The the data structure is slightly different, but the effect is basically the same. So it looks like we have a scalability trilemma here, right? So the trilemma claims that I mean, look, now, this is not an absolute, an absolute fact, right. And as I'm going to clean away their claim, there are ways to get around this, but it's very hard to get around us. And the trilemma claims that blockchain systems can only have at most two over the following three properties. So the first property is decentralization. And intuitively, it basically means the network can run as long as entirely on the basis of random ordinary guys with laptops, this is the definition of decentralization. The next. So, basically, you do not require data centers, you do not require like super nodes, you know, you do not require any, basically, any particular kind of, you know, cluster of that number, or any one category of user to have, you know, a more special a special status that other users do not have scalability, defined as being able to process more transactions than a laptop can process. So in this case, as you know, in computer science notation, O of n greater than ofc in security being secure against attackers with up to O of n resources. And the best way to interpret this as basically being secure against attackers that have up to some up to some fixed percentage of the network. So you have 51% attack for a naive proof of work attack, or you have 1%, you have 34% of percents to break. The asynchronous Byzantine fault tolerant consensus, you have zero to 34%, or 25% was the fixed that you selfish mining attacks, and so on and so forth. So what you do, what you're not allowed to have is a security, a security balance, or that goes down and or an attacker threshold that goes down and down, the higher the scalability goes, right. So it's actually fairly easy to get two out of these three. So we can have decentralization and security with a simple blockchain, you know, like Bitcoin and aetherium, as he exists today, we can have scalability and security with super big blocks. And we can have decentralization and scalability with 1000, all coins, can we have all three? Well, this gets very tricky.

Unknown Speaker  1:13:33  
So we're going to take a bit of a detour into something called interactive computation, or sometimes interactive verification. So the setup for the interactive verification game is like this, let's suppose that you have some function that you're so some long computation that you're trying to make. And this this computation is, you know, something that just takes an extremely long amount of time. So but it has the property that it's a function that can be decomposed into a large number of different functions, where if you take if you start off with some input x, and you know, you're the function itself becomes a composition. So f is a composition of f fN fN minus one all the way down to f1. So if you'll take a look at all the intermediate values f1 of x then out and then the value after f to the value after three, every single intermediate value is also basically like very small in size. So specifically at most ofcn size. So if you have a problem, this computation that has this property that it is just a long string of upper listen a value of being strong through a chain of functions, we are at every step along the way, the value remains small, then we can play this this simple kind of game. And basically we can give, we can kind of trustless Lee compute the value of f using a blockchain as a verification aid, but without doing everything on the wall. So here's the first step, the submitter is going to send is going to process you know, take x and run through F one, f two, f three, all the way down to fn. And the submitter is going to send a transaction, which is like a solution candidate, right? And the solution candidate is going to is going to contain, it's obviously going to it's obviously going to get to the end, why the answer, but it's also going to contain s one, which is the value that you get after you take x and you apply f one to it s two, so s one but applying applying f two to what s three, so as to but applying f3 to it and so on and so forth, all the way down to sn which is basically the same thing as y. Now, the submitter also sends a transaction, a transaction contains all these intermediate values in the submitter submits this along with a deposit. So first of all, we can immediately tell right now, if this scheme works, what is the maximum computational complexity of F that can be processed. So if this is done inside of a simple blockchain, then the blockchain has ofc capacity. So you know, the gas limit is ofc. And, first of all, what we're going later on, we are going to depend on the ability to compute any ephi inside the blockchain. So the computational complexity of each individual ephi can only be ofc. And because we have to submit all of these in one transaction, the number of these s values and so the number of F values is also going to have to be at most ofc, or c multiplied by ofc means that F can have a complexity of at most ofc squared. So what's the next step here?

Unknown Speaker  1:16:46  
Once the submitter submits the solution candidate, there is some challenge period, this could be an hour, it could be a day could be a week, could be five minutes, if you're feeling really daring. Within this challenge period, anyone can submit a challenge index. And the challenge index is basically saying, hey, look, this particular solution is wrong. And it's wrong right over here. So if, for example, you see, you know, some, some computation, and you just decide to randomly audit it. And let's say you randomly are, you know, going, you take the value s 42, that got submitted, and you just personally try to apply f 43, m, f 43. And you notice that the S 43 value that you compute is not the same as the 43 value that the guy submitted. So you notice that there was an error in the submission, you can send a challenge transaction, and as one of the arguments to the challenge transaction, so this is going to be calling a function. And as the argument in the argument is going to be this index 43. Now, what happens then is that the contract actually is going to execute f four, in this case, f 43, inside the VM inside consensus, and it's going to execute it on s four s 42. And it's going to see if it actually gets the same sF 43 that was supplied in the transaction. If it was, then you're just raising a false alarm, and you get nothing and you just wasted a huge pile of gas. But if the Yeah, it turns out the contract notices that this particular answer that was submitted actually is wrong, then the challenger gets some portion of this of the submitters deposit. And this is the incentive for people to do this kind of audited, if no challenges or challenges are made within some challenge period. And then the submitter gets the deposit back in the submitter gets the reward. And the system kind of assumes that the submission is correct. So this is kind of the basic idea behind interactive computation. Now, you can actually extend this, and there's two ways to extend it. So the first way is that if you wants to turn this into, if you want to increase the capacity, the genomic capacity from ofc squared to o of two to the two to the power of C, so make it exponential, then you can play this multi step game where basically, the submitter submits three values, and then the challenger has to say, either you're wrong on the left or you're wrong on the right, and then the challenger has to provide the value in the middle then the submitter can kind of contest the challenge and say either you're wrong on the lottery, you're wrong on the right. And so you do this sort of interactive binary search until eventually you get down to the bottom and on the bottom, you do it on chain, so you can do this sort of thing. And another thing that you can do is you can also actually extend interactive computation so that it works with a computations where the values in the middle are too big to integrate basically bigger than ofc. So you can select put the entire intermediate results onto the blockchain all at once. And the way you do this is with Merkle trees.

Unknown Speaker  1:19:55  
So in

Unknown Speaker  1:19:58  
my when I presented this since San Francisco required everyone to bow to Ralph Merkle. So we'll require everyone here to do the same. So everyone should be really thankful for this guy, because you know, it really is, you know who here has synced in aetherium light client, who here has synced a gift full note and anytime in the last year, he already note in all of you have saved hundreds of gigabytes of bandwidth because of tech, this guy.

Unknown Speaker  1:20:32  
Okay, so

Unknown Speaker  1:20:34  
for those who don't know what Merkle trees are basically, they are they allow for efficiently verifiable proofs that basically some piece of data is a member of some much bigger piece of data. So if you want to make this much more abstract, you can assume that let's say you're having one megabyte file, you're going to make a this kind of complicated hash construction out of the one megabyte file, and eventually, you're going to create a it's going to have a 32 byte root hash at the top. And so you might want, what you might want to do later is you might want to prove that some particular value is in some particular position in the original data, when the original data is kind of publicly represented by the root hash. So what you do is you basically have all of these, all of these hashes. And basically, you provide a the Merkel, the Merkel proof, or sometimes called the Merkel branch, is just a set of hashes that goes from the root, specifically down the chain, going all the way going along the this kind of path to the specific value that you're trying to give a proof of. And so someone can check the proof basically, by checking that the hashes match up at every step along the way. And if they match up, then the proof is correct. Now, if someone tries to create a false value is medical miracle, provable false value, then at some point, the hashes are not going to match up. So this is what Merkel proofs do. Now, the idea basically, is that and you know, Merkle trees actually get used a lot in aetherium, in the other major use case in aetherium, is that they use them for the state trade, right. So the state in aetherium is basically the set of all accounts and the set of all contracts storage for all contracts. So basically, all of the kind of information that you would need to have in order to process and verify transactions and just know what the state of things is right now. Now, this is stored in this in this kind of special hash tree, and any particular state, it can be represented by this 32 byte route hash that we call the State Route. And so state routes are, you know, unique, basically uniquely maps to the state. And so if I have the State Route, then theoretically, I can use that to authenticate the entire state. So all of the accounts, all of the every single piece of storage in every single contract all of the balances. But in order to authenticate any individual piece of the state, I would need a Merkel branch and a Merkel branch basically just as the hashes going down from the root all the way down, down along some particular path in the tree. So

Unknown Speaker  1:23:13  
the way that we can think about fraud proofs is that fraud proofs kind of are an application of interactive computation to verifying the blockchain. So but here is kind of what I what I mean by this. So you can think of verifying the entire blockchain so verifying a blockchain and you know, from Genesis to a 4.33 9 million where it is right now, as kind of like being an F, this f function, right? So it's in it's an F, that decomposes into f one.fn. very naturally, x is the Genesis state. And we can think about the you can think about the state as being the State Route, right? So you can kind of think about the state of the state here is just being the route hash f1 is apply bought the first block to the Genesis state, what is the state after that? f two is take the state after block one and apply block two, f three as applied block three, and so on and so forth until fn is apply, like the last block in the chain. So right now block goes to 4.2 9 million. Now, there is one important a one important difference here, though it's between verifying a blockchain and this simple case that we talked about earlier. So what was verifying a blockchain? Right? So first of all, what do we know that what can we kind of allow users to have, right so you can require like clients to download every single block header, right, so every block has this like piece of data at the top called the block header. And the block headers purpose is basically to contain the reference to the previous block and to contain a whole bunch of data that are in the container, a route hashes. So a hash of the transaction tree, a hash of the and a hash of the state of this of the state, the so the the root hash of the state after processing all those transactions. So we can assume that every year, you know, let's say, we have a type of user like clients that can download all of these, all of these block headers, but they do not have enough capacity to download and process all of the blocks. And so what we do is we're going to basically do a kind of a very similar thing, what we're going to say is that a light client is just going to actively listen on the network for challenges. And what is the challenge? Well, a challenge basically says, walking, you know, in this project in this chain that you got blocked, number 3,849,223, is wrong. Now, the main difference here, though, is that you because you're operating over trends, large sets of transactions, and because you're operating Overwatch states, in the light clients by itself is not going to store the state. And it's also not going to download all of the transactions. Basically, a challenge has to also provide witness data. So what do we mean by witness data, basically, the block that Litecoin already has all the block headers. And so the light client has the Merkel tree root of the transactions in the blocks and of the state after processing each block. So you can think of that as being like these si values. But what they do not have is a delay client does not have the actual transactions. And the lightweight does not have any portion of this of the state, they're in any block. These are values that the light clients can authenticate, if you provide them up with Merkel branches, but these aren't values that the Litecoin has yet. And so a challenge has to be not just the challenge index, but also witness data, right. Also, the Merkel branches that allow local that basically tell you, you know, this is the actual set of transactions in this particular block in this and this is the portion of the state that was modified by this particular block, so that the lifelines can actually run through all the computations, and check whether the State Route after processing the block is correct or incorrect. So because you're operating on these large sets of data, and because so because it's not like the, from the kind of like client's point of view, it's not a pure computation, it's this kind of computation that keeps on constantly accessing this extra data from the outside, you have to like challenges also have to provide this witness data as well. Now.

Unknown Speaker  1:27:45  
So what is a fraud proof? Basically, a fraud proof is a challenge index, plus all of the witness data, so the transactions and they're kind of branches in the state that you need to execute a particular block. And so like clients can execute, if it receives a challenge, he can actually verify only that particular block, and he can check for itself whether the block is actually valid or not. Now, this is so far, theoretically doable, but there is a problem. And the problem is data and availability attacks. So then on availability attacks are basically attacks where a malicious miner creates or publishes a block that the block header is present. But some or all of the block data is missing. Right? So you can think of this as being a block that has a transactions. Well, the first transaction here, it could be a valid transaction, it could be an invalid transaction. But whoever published the block is not publishing it. Right. So now, first of all, what can data unavailability attacks? Do you know what is the worst that can happen because of a data unavailability attack? The first thing that the attacker can do is they can convince the network to accept invalid blocks. And particularly, there's no way to prove invalidity, right? So this transaction over here could be a valid transaction, or it could be an invalid transaction, that gives me 50 million ether out of nowhere. If you do not have the data, you have no way of verifying which is which. And technically, in an information theoretic sense, there's no there is basically an infinite number of valid transactions that have the same hash, so it really could be any of them. And there's also an infinite number of invalid transactions that have the same hash though it could be any of those. So that's the first thing that you can do. But now, there you might think, Oh, well, we have you may have heard of weird, spooky fancy math called z k snarks, zero knowledge proofs or z k snarks, and these basically let you prove the block is valid. And let you prove that you are what we're more specifically what you prove the block header is valid without reveal actually revealing any of the data Right. So you might think, Oh, well, we have this kind of fancy cryptographic magic, why don't we just let the cryptographic fancy magic proof for us that all these blocks are valid, and that we don't need, we don't need to care about the data? Well, it turns out that even in that case, there are other very nasty things that data unavailability attacks can do. So particularly, if you prevent someone from learning about a transaction, then even if they have this, the new State Route, and even if they have this magic cryptographic proof that the new state route is correct, what they do not have is the entire new state, they have no idea what the balance of certain accounts is. Now, they might be able to download some of the branches, but they're not going to be able to download all of them. And so what the data on available ability attacker do is basically, you know, if it could just prevents people from knowing what the current state of certain accounts is, and what the even nastier thing that happens after that is that if you do not know what the state of some account is, then you cannot create your own transactions that interact with it. Right? So the network, how if an attacker makes this one invalid block, or Oh, sorry, this one on partially unavailable, block publishes it and if this gets accepted, then this attacker disappears, ever, all the other participants in the system are not going to be able to agree on blocks that contain transactions that touches affected accounts, because nobody has the information that they need to make a cryptographic proof. So the cryptographic proof could be a Merkel branch, it could be some kind of fancy witness scheme. It could be you know, it could be whatever. And this applies to aetherium, like account systems that applies to Bitcoin, like UT EXO systems that applies to, you know, like, basically any kind of design. So Does everyone agree that data unavailability attacks are bad?

Unknown Speaker  1:31:56  
Who here thinks that did on availability attacks are good? Okay.

Unknown Speaker  1:32:04  
So the data availability problem, right? So incorrectness can be proven even to Litecoin. And this is basically done with fraud proofs, right? Even if you cannot verify directly, you can always verify indirectly, but data unavailability is not like fraud, or is not like fraud in the sense of creating invalid blocks. And the main difference is, the data unavailability is not a uniquely attributable fault. So what do we mean by uniquely innovative all fault? Basically, it's a false where, if it happens, you know, regedit. Now with an invalid block, there, it's totally attributable, because if someone creates an invalid block, while there's, you know, in proof of stake, you have a signature attached, or in proof of work, you have kind of this one time identity inserted with the with a proof of work solution. And you know, that, you know, it was this miner that caused the problem, or that it was this, it was this validator that caused the problem. And so you can deny them or award or you can penalize them. And, you know, this gives you all the right economic incentives to disincentivize them from doing any sort of nasty stuff. Now, we'll see an unavailability of the problem is basically this. So, let's go through two possible cases. And these are just two possible sequences of events that could happen at some point. So here's case one, at time t one, validator v one who is evil because he has horns. Anyone here have horns on and are not evil.

Unknown Speaker  1:33:35  
Okay, I can be racist against torn people.

Unknown Speaker  1:33:38  
So validator v1 publishes a block with missing data. Right? So validator v1 publishes a block. And this one tiny piece of the block is just not there. It's missing. It has not been published against the network, or published to the network, time T two. validator v2 raises an alarm Ray basically says, Hey, this piece of data is missing log log, well, I'm afraid at time t three value review, one goes ahead and just publishes the remaining data. Right. So basically, a validator just says, okay, fine, you raise the alarm and publishing the rest of the thing. Now, here's case two, validator view, one, who in this case, no longer has Horton, or the dog or has horns, publishes of luck. And the block actually does contain all the data at time T two, validator v2. So the challenger who is now evil, raises a false alarm. So basically says, look, look the status missing when it's actually there. At time T three, in this case, validator view, one just does nothing. Now, here's the interesting thing about this story, from the point of view of any client that log that logs on or that that appears or that starts paying attention to this piece of data after time t three case one and case two are completely indistinguishable from each other. So, after times three, all that you see is there is a full, fully published blog, and there's an alarm, but you have no idea whether it was case one or case two that gave rise to the situation. So this leads to something that I call the fisherman's dilemma. And it's called the fisherman's dilemma, basically, because fishermen is a technical term for V. Avella specialized nodes in the network that do this. Right. So the fisherman's dilemma basically says in let's suppose that we are living in case one, what is the expected return of validator v2? And there's literally three cases, right? There's three kinds of real numbers. So there is some real numbers corresponding to people who can count in real numbers corresponding to people who can't, that was a joke. No, so there's real numbers greater than zero real real numbers equal to zero, and the real number is less than zero. So first of all, real numbers greater than zero, right? So what if the expected return of v2 was positive? In this case, what this actually means is that there is a money pump vulnerability, right. So what the money pump vulnerability means is that in case to basically validator v2 also makes a profit. And so what this means is that there's profit to be made by just going around and raising false alarms all over the place. This is bad. Now, let's say it equals zero. Well, if it equals zero, then there's no money pump vulnerability. But what there is, is it's an off service vulnerability, because validator v2 can just raise alarms for every single chunk of every single block of data until everyone is forced to download everything. And if everyone is forced to download everything, then that contradicts our goal of scalability, because, you know, we're assuming that each individual client has ofc resources, and they're being forced to download to download more than ofc data if the blockchains capacity is more than ofc. Keys through the other cases, the expected return is less than zero. So v2 loses, you know, that's good in case two, because, you know, it means that it's costly to do this kind of attack. But in case one, what this means is that there's no incentive to first of all, there's no incentive to raise the alarm. And second of all, there's a disincentive against raising the alarm. And so what an attacker can do is basically, they can just keep on publishing blocks with missing data, they can wait for they can let the altruistic challenge, challengers keep challenging, then they could just outlast the altruistic challengers. They could just wait until the altruistic challengers run out of money. And when they run out of money, they can keep publishing blocks with missing data. And these blocks could be valid or invalid, and the clients will just give up and accept them. So this is kind of you know, this is kind of why a fraud proof like approach to data availability is just not going to work.

Unknown Speaker  1:38:05  
So there's two main categories of solutions to this. The first category is you rely on an honest measure or on an honest majority assumption. And you use random sampling. So basically, the idea is that for like, you can imagine a blockchain that's kind of, you know, a shorted blockchain. So you would have like 100, different kind of universes. And these would be 100 separate sets of accounts. And well, then each university would also have 100 separate groups of transactions. And you would just randomly assign some group of validators to, you know, verify avail validity or even just verify availability of, let's say, everything on shard one, or everything on shard to everything or, you know, you might create a committee for shard 43, basically create a separate committee for each individual shard, right, or for each individual kind of universe of accounts and transactions, then, then basically, everyone would just trust the committee, right, so each committee would be responsible for downloading all of the data. And if, if the committee says that the data is available, then we basically just trust that the data actually is available. So this is one solution. But this crucially relies on an honest majority assumption. The other kind of solution basically has to do with client side random sampling. So client side random sampling, the idea basically is, you know, select, you have this big hunk of data, and you only have that you only have the while each individual client is only has like the root hashes of the data. And so the clients basically tries to determine if that is if the data is available by doing random sampling, right? So you just randomly select a bunch of indices, try downloading some transactions that have those indices, you know, just randomly pick index number 623. Try to download transaction number 623 Pick index to 85 try to download transaction 285 do this 100 times, if this passes, then it looks like all the data is available. So this works well against attackers that try to withhold all of the data or against attackers will try even attackers to try to withhold half of the data, what it does not work well against his attackers to try to withhold a tiny piece of data, right. So if an attacker withholds only one single transaction, then ran, this kind of random sampling is not going to work. And so you're basically going to have to like a client is going to have to download everything in order to verify that there isn't a single missing transaction. And as we all know, a single missing transaction is basically enough to completely wreck the system, right, because it with a single missing transaction, you know, that transaction could be an invalid transaction. And I could give myself 50 million ether. Fortunately, you know, we have erasure codes, you know, error correcting codes, basically a bunch of fancy math that specializes in turning 100% availability problems into 50% availability problems. So the usual mathematical explanation, I give her this as, if you have a piece of data, then you can encode the data as to as two points. So you can encode the data as two points. So you'll put on a plane, you draw a line going between those two points, then take a bunch of other points on that line. So any two points on that line, are enough to reconstruct the original line, and therefore any tuple any of those two points are enough to reconstruct your original data. So this is how you solve a kind of two of two problems, or you turn into a two problem into a two a four problem. Now, if you go from lines to a high degree polynomials, then instead of it being too event, you can use make it be 1000, or 2000, or, you know, 10,000, or 20,000, or like whatever exponents you want, right? So

Unknown Speaker  1:41:58  
what they're basically the idea here, then is that, you know, basically you if you have a block, we have some transactions, and the transactions are for a Merkle tree, then you would just make another Merkel tree out of this extra data that gets added. So you can think of this as being like these extra points on the ball on a polynomial. And any, like any 50% of all these values together are enough to recover the original data. And so clients can just like sample for this to determine it's a sample through this in order to kind of probabilistically get an idea of availability. Now, in order for this to work, you also need to add another kind of fraud proof. So the other kind of fraud proof would basically say, this, or Oh, look, for this particular block this or the erasure code is actually malformed. And it's malformed because you know, you know, look, if you actually try to reconstruct the whole thing, then you get an inconsistency. But if you have that, then you can kind of use this mechanism to verify that that data actually is available. This is kind of similar to proofs of retrievability. And like a lot of similar math is used. But the main difference here is basically that in a proof of retrieve ability, it's Alice sends Bob a file. And then Bob, that's, and we assume that Alice is trusted. So we assume the prover creates the file is trusted and be severe, the ones that send a challenge or possibly a third party that sends a challenge, and Bob has to reply back with a valid response. Here, the main difference basically, is that the person who is creating and uploading the file is also untrusted. So but you know, or alternatively, in this case, Alice is Bob, but you have third parties and arbitrary third parties that once guarantees about the availability of the file, and to build the wall forgiveness of the ratio code and also about, you know, is the file actually correct according to the rules of the protocol. So they're in there are are actually some in it is kind of computationally viable, a lot of this still can be improved much more. But, you know, it's another important kind of path forward is trying to combine a lot like a lot of this stuff with zero knowledge proofs, because with zero knowledge proofs, you can remove the need for fraud proofs. So this is a kind of more high tech direction for verifying data. Data availability, as I can, you know, doesn't rely on trust assumptions as much. Now my opinion is that if you want a maximum security solution, then you can basically layer these two schemas on top of each other. And so you can say, you know, a regular node should accept the block is finalized, if, number one, the committee says it's okay. And number two, it's valid and stubborn or sorry, and number two, the sampling says the data is available. And number three, it hasn't seen a fraud proof yet. So if all three of those checks passed, then you know, and that gives you a quite a strong guarantee that the data actually is available and valid. So This is probably the closest, the closest that it seemed that it seems like you could get to kind of verifying availability and correctness of data much larger than ofc with only ofc resources. And But, so, basically with this kind of technique, you can view you know, you could imagine a blockchain where the total capacity of the you know, you also have a shortage system. So, the kind of transactions are split into these different universes. So, it can be executed in parallel on different nodes. And the total capacity of the chain might be something like ofc squared, because you know, you have ofc, different shards and each shard has ofc capacity, and then you have like a top level mechanism that keeps track of all the shards. So, he has, so, you have ofc of those in total. And then but we are each individual node does not need to have more than ofc processing power and we are individual clients also do not need to have more than ofc processing power in order to stave out verify any one particular shard. So, this is like, if you wants to increase the kind of base layer capacity of a chain from ofc to MC squared. And to do so, while preserving all these, like these kind of decentralization and security properties, this seems like the sort of direct in the sort of direction that it makes sense to go in at least if he wants to guarantee you know, availability and correctness. So thank you.

Unknown Speaker  1:46:31  
excellent ideas. I mean, so now, I guess we're we have a panel lined up. So we could actually just jump directly to the panel, where we'll get to an opportunity to ask retallick more questions. So why don't we do that? So all the speakers, please come up. And we'll sit around the table there. And I want to remind everyone, when the panel is over, we'll, we're gonna go out and have there's gonna be food outside and kind of have informal conversations. So let's get started with the panel.

Unknown Speaker  1:47:22  
All right, good. Okay. So

Unknown Speaker  1:47:26  
I guess maybe I could start for a few minutes, I,

Unknown Speaker  1:47:30  
there was actually a big conference deadline. So I was not able to prepare a talk. And I'm not going to give a talk. But maybe I could just start with one or two minutes of stories, and then we can dive right into the panel. So I just wanted to tell to kind of share some experiences with everyone. So we've been running a blockchain cryptocurrency class here, for a couple of years, it's been a lot of fun, actually, half the course is devoted to aetherium, where the students write all sorts of contracts and play with them. And I want to just share one experience that I had with you, I think, I think it's kind of an entertaining experience, and how hard how hard it is to write correct contracts. So what we do is on our final exam, we give students a contract where we deliberately put mistakes in the contract. And the challenges, you know, find all the mistakes, the more mistakes you find, the higher your higher your score is, we don't tell them how many mistakes There are eight. So that's, that's kind of a reasonable challenge for for a final exam. Now, we qualify our exams actually very carefully, right, we run them by the TA is we write them, run them by other students to make sure that really, the contract we wrote has the bugs that we intended to and that those are all the bugs that are in there. Yes. So lots of people looked at this contract. Yes, it was a simple, simple game. I think it was like two pages of code. Lots of people looked at it, we made sure that all the bugs that we wanted to were in there, we gave out the exam. And you can pretty much imagine what happens, right? The exam came back with solutions. Some people found many of our many of the bugs that we put in, but there were many students who were able to find bugs that we never intended. Right. So that was that was actually kind of a scary lesson. To me in that, you know, you can have multiple people look at a contract, and still the contract is going to have bugs in it could get stuck and could do things you never intended. So it's I think it's kind of a challenge to the community that, you know, given the importance of these contracts, it's a kind of a challenge to the community to develop usable, usable emphasis on usable tools for verifying for verifying contracts. And I think this is a really fruitful area of research. So one of the reasons I'm really excited about Metropolis is because of the new parent capabilities that are going into the into the field. This is really, really, really cool. So that's been my dream to have pairings out into the wild. And I'm really happy to see that Metropolis is making it real. Now, I think the motivation for Metropolis is that it now supports, you could do snark verification in a contract, so you wouldn't you don't have to pay that much gas. And still you can verify these proofs very efficiently. But the beauty of the API that vitalik and aetherium the theorem engineers have Design is that it doesn't just support snarks it actually supports a much broader capability. And I think this is important for everyone to understand now that parents are going into metropolis. Inherently in the in the EBM, you can actually verify BLS signatures. Yeah. So the mechanism that's already that's going to be available is ideal for verifying BLS signatures, the API supports it natively. And let me just remind you, BLS signatures are signatures that are generally quite short. But more importantly, these signatures can be aggregated. So literally, you can take 1000 signatures, and squish them into a single signature, and verify that single signature authenticates all the thousand matches messages that it was applied to. So these are different messages signed by different people can all be squished into a single, single signature. And now that's supported natively. So think about starting a new blockchain, you can actually take all the signatures in a particular block. And instead of wasting space and putting those signatures, one by one in the block, you can compress them all into a single signature that's really short. And that would verify all the transactions in the block. So that's actually now natively supported. It's called BLS signatures. definity is actually putting this to good use. And I hope more of you will be able to do that. But again, once bearings are available, they enable a whole host of applications beyond just z. k. snarks. You can imagine now a new new forms of group signatures that are based on pairings new forms of broadcast encryption, that's based on parents is lots of lots and lots of new capabilities that are becoming available. So I think that's, that's really exciting. And I'm really looking forward to seeing more contracts that, that use this. Okay, so maybe I'll just stop here. Maybe I'll just tell you that one more thing, one last thing. So I love confidential transactions, we actually have a lot of work that we're doing on confidential transactions, making them more efficient, more secure. And that will be coming out shortly, actually, hopefully, we'll wrap it up by the end of this month. Okay, so let me stop here. And with that, I'd like to start the panel. So my first question for the panelists is, let me try to generate some controversy. So you two guys seem to have contradicted one another. So vitalik, you had your trilemma? And plasma seems to be a counter example to the trilemma. So maybe you guys can duke it out and say which way who wins here? Yeah. Plasma, scalable, secure and decentralized.

Unknown Speaker  1:52:31  
Okay, so I would probably, so what I'll probably this is probably the best way to answer this as to go straight into what I think is the main, basically the main kind of security, like sealing of generally interactive and channel based systems, and the thing that stops them from being used dog to like, literally, you know, controlling tons of transactions a second. And this is basically the risk of what's called a mass exit attack, right, or an electric bass challenge attack. And the idea is that imagine, I mean, this could even be explained, like in the context of something like the lightning network, imagine you're a big node in the lightning network, you're someone like Coinbase, and you have channels with a million users. And let's say, you know, you have, you decide, you decide that you're going to attack all of them, or you get hacked, and the hacker attacks all of them. So the attacker is going to try to cash out of all these channels all at the same time. And you have, you know, like basically, before the challenge period to challenge or every single user has some challenge periods to challenge them. But, like, basically, there literally isn't enough block space for anyone's for everyone to challenge them before, like, the timer is run out. For me, some of them, like, the reason why this attack is nasty, nasty, is that it has this kind of systemic risk character to it, and that everything feels like it's working fine. And so it's not, like it's, you know, an attack would just come very suddenly, and you know, if it has an effect, the effect could, could be fairly large. Now, there are a lot of mitigations and, you know, Joseph thought about thought about a lot of them, other people have thought about a lot of them. So one of them, for example is basically that you could have, like the timer's automatically get delayed, if other challenges are happening, like basically like I think there is a like there, there is some like fixed ratio between the capacity of a chain and the capacity of a, or in the capacity of the second layer of second layer systems where if you go beyond, like, if you go beyond that difference, then like that, yeah, there is a challenge periods need to get it needs to get longer and longer for the thing to say. So that's kind of the dimension you end up compromising along.

Unknown Speaker  1:54:47  
Yeah, definitely don't agree with a lot of those. No, because a lot of those are all specified in the paper, you know. So the the main thing I mean, if you want to contextualize this things within you know, buckets of money frameworks that doesn't really disagree necessarily with this trilemma is the the dynamic is that a lot of the security properties can shift in terms of, you know, it can be fairly efficient, it provided that, you know, the dynamics are this and in the event of failure, then the capacity goes down of the network, right? In the event of failure, you can say, Okay, yeah, one of the one of these three properties do not get as, as robust, whereby, you know, perhaps you have some type of time soft constructions, or pre reserved space in the event of disputes. So you could say, like, the capacity is very high, and perhaps it slows down in the event of disagreement, but everybody else continues operating. So there's a lot of, you know, it's, it's not so much as one hard thing where it's like you're making, you're picking two out of three, but it's more, you know, perhaps you get everything. But in the event of failure, things slow down. So the two out of three is speed. Additionally, if you want speed, in addition to that, you can select centralization, in addition. So it can be done in layers, whereby, you know, at the route layer, you can say, okay, you trade off some speed. But if you still want speed, you can have centralization. Above that, with a backstop of, you know, slow down and speed. The idea is that, you know, you have in the smart contract, you have, you know, some type of time stop. And above that in plasma, you have, you know, the hierarchical chains. So there is some quote unquote, trust in Node selection, and you can remain robust in that way.

Unknown Speaker  1:56:28  
But we haven't really result so you're saying?

Unknown Speaker  1:56:32  
Security is not as effective as a

Unknown Speaker  1:56:36  
security is not as effective as what

Unknown Speaker  1:56:38  
is not as effective as by a massive exit?

Unknown Speaker  1:56:42  
Yeah, you can mitigate it in in, in multiple strategies. Yeah. To to a degree, which you're comfortable with, especially if you do parent chain selection.

Unknown Speaker  1:56:51  
Yeah. Cool. So, so I'm gonna ask Joseph,

Unknown Speaker  1:56:56  
in one minor point, one minor point is, is that that becomes a one event problem, and not an mF n problem. Because, you know, if as long as one of the parent chains are robust, right, then you have ability as opposed to, you know, a federated construction where, you know, you want the majority to be robust.

Unknown Speaker  1:57:13  
So plasma kind of relies on the Ethereum right, this the basically the chain itself, but it uses it in a very, very, very specific way. Right, it's very specific way. Could you imagine building sort of a blockchain that would be like your route of trust? That is a mechanism that's actually a lot simpler than then aetherium? Or do you need the full power of aetherium?

Unknown Speaker  1:57:34  
Um, so my personal opinion is, I think you can do it, it depends on what what your use cases are, right? Like, if you want to do it strictly for payments.

Unknown Speaker  1:57:43  
You can design a construction that's like much, much, much simpler.

Unknown Speaker  1:57:47  
It also devolves to lightning basically. Um, well, because it's interesting, because like, there's this notion of, you know, the concept of, you know, cryptocurrency reserve currencies. There's a lot of discussions about that as replicating gold. I think history rhymes right? You want. If you view reserve currencies as important, it's possible that the true cryptocurrency reserve currency is a system that allows you to use that cryptocurrency on other blockchains. And that's a property that perhaps plasma could incidentally allow, and could be an important construction. So lightning doesn't inherently give you that. And plasma is, if you design it a certain way could hypothetically give you that? Yeah.

Unknown Speaker  1:58:32  
So it's plasma is a sort of decentralized exchange.

Unknown Speaker  1:58:35  
If you're using a coin from one uses, you could use a decentralized exchange, but in this construction, you can for example, you know, hypothetically, let's say, let's say Bitcoin implemented it, right? You could hypothetically then if you designed it in a certain way, use Bitcoin on aetherium. Right, now, you can write, right now you can use aetherium on other chains. So for example, you can hold, eth, on etc. And that way that can be a reserve currency, because you can use that as a sort of a sort of like, you can you can use that on a transactional basis and use some other computational system.

Unknown Speaker  1:59:14  
Good. So vitalik, you have a huge crowd here, who are very, you know, interested in fascinated by aetherium. Do you want to see a few more words about you kind of chose a very specific topic to talk about in your in your talk, which was really interesting. But maybe you want to say a few more words about how you see if you revolving over the years and where where is it headed?

Unknown Speaker  1:59:34  
Yeah. So I think like from a technical perspective, it's I mean, Metropolis is obviously the, the immediate next step, although the kind of story of using cryptography in inside of Ethereum doesn't end there. In fact, you could even say it starts there. Because, like, we I mean, getting the actual breaking policy into a theory M is like is a prerequisite for actually doing Building all the infrastructure around applications that take advantage of that technology. So, you know, we have will have Zika snarks, but you still need to make the Z cash token, you know, is still or a z cash like token, you'd still need still, you know, do the tooling for things like anonymous voting or like a various privacy preserving identity systems, you know, you'd still, if you building ring signatures, you still need to design the systems around that, you know, threshold signatures, design the systems around that, there's still a lot more kind of schooling to be built there. And like one of the holy grails I think, that a lot of people are trying to think about is, can we design a programming language that can specify kind of the degree of privacy of different variables and different events, and just automatically compile things down to the right kind of cryptography? So like, this is, I mean, this seems like this sort of project that could be interesting to try to, you know, like, how you design with a theory and as a target. This is a cryptographers dream? Yes, exactly. Like, it's of your, I'm sure it will be quite in and take a while to get there. Then aside from privacy, there is also like all this stuff around scalability. So this is like a combination of basically, you're scaling stuff, like I'm in plasma, like other state channel constructions, it's probably the those are probably the kind of three main categories of things that people are thinking about, then I am regarding security. So we are trying to move toward this idea that we have called signature abstraction. And the ways basically signature abstraction, the best way to think about it as like, your public key becomes a program that takes as input a hash and a signature and returns zero and one. And so, you know, within this kind of system, you could you, each individual could use whatever signature scheme they wanted. So, you know, you could switch that, like any of you could independently switch to hash based signatures, threshold signatures, like cry, you know, some Zika snark based based system, like basically, a multi multi signature sharing or whatever you want. But we are in, you know, this would still be kind of standardized, and you could use it inside a theory on either for to secure your own accounts or to, you know, to assign assign messages in group stake, right, as a replication program would be written in a VM. Yes. Yeah. So that's, I mean, then for proof of stake, and that's obviously a big, major set of milestones in its own right. And

Unknown Speaker  2:02:37  
it's,

Unknown Speaker  2:02:43  
we're gonna stop there. So, okay, that's a leave outlines quite a lot of this many years of work that you just described that. So we can stop there. So apologies a question for you. So where do you see your your coin going? Going in general? Sure.

Unknown Speaker  2:03:00  
Yeah. So, you know, one thing that I think most folks in the Bitcoin space learned over the last couple of years is that, because on a technology platform, it's an awesome investment. It's a technology breakthrough, it's very hard to build things on top of it. And so what I'm actually quite excited about is, you know, we think that sort of the next step, right now we've had a relatively small number of people have been able to mine cryptocurrency and then a large number of folks would be able to buy cryptocurrency. And what we think of as one of the big next steps is to allow millions and millions of people to earn cryptocurrency. And so that's what we're trying to do, where it's actually innovation, hopefully at the application level, rather than the infrastructure level. So to say, we're not using every single sophisticated feature that we've got the watching level, we're just trying to do that very simple thing of onboarding a new user into digital currency, which itself actually has lots of useful implications. Because once you onboard users digital currency, they've got a private key. And they know to keep that private key secret in the event incentive to keep it secret, because if it's compromised, they lose their, you know, digital currency. So all this stuff that dependent upon distribution of private keys, the whole public key infrastructure problem actually gets solved. If you can successfully figure out a way to onboard millions and millions of people into digital currency without losing your shirt, right, you don't just give away the money, you want to figure out some way that they are actually doing something for it. Now, one of the methods up to this point for the distribution of digital currency has been, you know, the Ico where, you know, a bunch of folks come and they send in, you know, Ethereum or Bitcoin to get back, you know, tokens, and that's great. You know, I'm not anti Ico, it's actually a remarkable thing, you know, this year. But we think that a very interesting new way of doing things is to send in labor rather than capital and receive a token back, and or, you know, more than one. And the utility of this is, it may be a new way to bootstrap social networks in two sided marketplaces for a long time where people have talked about is, hey, what if you could have taken half of Facebook's equity and split it among all Facebook users. And if you did that, that's like $250 billion divided by 2 billion people, which is $125 for every man, woman and child on Facebook, which is actually non trivial amount of money. But in fact, he wouldn't do that in an egalitarian way actually wouldn't do it in a completely equal way. Instead, what you do is you'd give some larger incentive for the first and the 1000 and the 1,000,000th user than you would for the 1,000,000,000th user because that person contributed more value to the growth of the network. So if you have a sloping function like this, the one that we've chosen is actually very similar to the Bitcoin block reward, where every time the user base doubles after initial threshold, the token reward halves, SEC has a very strong incentive for early adopters, and maybe a completely new way to bootstrap a social network. And in addition, if folks invite each other, then they also get tokens and so on. So if this works, you know, we know that you can monetize social networks, if they get to scale, anything at scale is interesting. If you've got 10s of millions of users, there's standard techniques you can use. And there's existing things like LinkedIn in mail in particular that we would be competing against. If this tokenisation method works. It may be a new thing not Icos, but a way to bootstrap a network, a social network or to set marketplace via labor rather than capital

Unknown Speaker  2:06:07  
to really Icos for social for building a social network.

Unknown Speaker  2:06:11  
Well, it's not an Ico is a token. token. Yeah, exactly. tokenisation bananas. Yeah, that's right.

Unknown Speaker  2:06:16  
So actually, I'm going to open up, I'm going to open it up for questions in just a minute. But I do have to ask the panel, maybe you can just comment a little bit about what you think, the impact on cryptocurrencies, the, I guess the Chinese actions are going to have an impact on cryptocurrencies, we have to discuss that we can't end the day without discussing that. So

Unknown Speaker  2:06:37  
who wants to go first?

Unknown Speaker  2:06:38  
Realistically, I just think that we'll end up seeing less cryptocurrency projects inside of China and more cryptocurrency projects outside of China.

Unknown Speaker  2:06:55  
Well, I mean, in some ways, this is sort of the thing, the moment that crypto has been building towards, so if a government banned, Bitcoin and aetherium, and so on, and it was finished, then the whole thing would have been in vain. So this is actually the whole moment, everything has been kind of building for. I think a few things are interesting to see how this plays out. First is, so there is this block stream satellite, and it doesn't yet have overflight over China. But if it does, that may be a way to get the blockchain into China. But one of the most interesting things I think, is going to be, you know, the partition tolerance in practice. And also, eventually, maybe, you know, it's a research project of blockchains. Because right now, they assume global consensus. But if you could imagine the Great Firewall getting really good and serious penalties on people for sending the blockchain back and forth, you might have a period of peekaboo where, you know, mining extends a blockchain within China. And it also extends outside and they can't sync up really in time. And so transactions get messed up and so on. Right. So I think partition tolerance is like a really important kind of problem for us to deal with. First, it's going to be dealt with in practice, and then hopefully, in theory. And yeah, unfortunately, right. So this could be, you know, it could cause a real crash in the utility of crypto for a while, depending on how aggressively they go. If they nationalize mining, you really would get the 51% attack wouldn't be a theoretical thing, you'd take 51% of mining. And I think we should just psychologically prepare for Worst case scenario for a while. And then, you know, you're always pleasantly surprised.

Unknown Speaker  2:08:23  
Um, I just flew back from Shanghai about two days ago. And my perception is, is that, you know, there's, there's an understanding that, you know, from the developers that there's a possibility that, you know, the shutdown is real, and they're very, very willing to go overseas, and you probably will see that the biggest problem in this space is developers, it's not, it's not money, you know, this washing money, the space. And in order to get up to speed, it takes about a couple years, you know, like, the fastest people I know, to get up to speed when it comes to, you know, understanding the code, understanding the mechanism, design, understanding the economic incentive, fastest I've seen is about a year and a half before that was probably three years, right, in order to get up to speed. It's definitely getting better with education. And education is doing a great job with this. But it's still a huge, huge, huge ramp up and is like pretty intimidating for a lot of people getting started. So, you know, like, there's a possibility that all you'll see it China's enterprise blockchain projects, and the people working on enterprise blockchains are like second tier at best people building open systems are, you know, at the forefront and really understanding the problem, because like, you know, building enterprise blockchain is sort of like, you know, playing house, you know, you're you sort of like, you have people saying, Oh, well, you know, we're the trusted authority, if we screw up is no problem, right? And like, you know, we just run a database. So like oftentimes, the enterprise blockchain projects are not really blockchains in general anyway. And they're not really building the core technologies and understanding and reasoning about mechanism design and incentive and economic models correctly. So the biggest problem with that what's going out of China is that it's basically holding them back probably about two years just by this auction.

Unknown Speaker  2:10:09  
Sounds to me like it's basically a new application for Tor. It's a way for people to now transact transact on

Unknown Speaker  2:10:15  
Yeah, probably currencies with external exchanges. Yeah,

Unknown Speaker  2:10:17  
yeah. And I will say that if people are like, as far as like, like censorship resistance goes, I think it's important also to just be realistic about what you can shoot for. So personally, I don't think people need like, a way of downloading the full blockchain or running a full node. I think there's this like, point like pointless and like for, like, a lot of people really greatly over exaggerate the importance of that. I think what these what you know, people need is just more interfaces by which they can get by which they can read data from the blockchain and send transactions and just like, copy paste, like a blob of hex into a into a Skype chat. And you know, there you go, it gets rebroadcast on ether scan, like that's really, you know, the kind of 10% of the work that gives 90% of the

Unknown Speaker  2:11:05  
effort here.

Unknown Speaker  2:11:08  
Cool, very good. So on that note, we have time for a few questions, and then we'll end and have kind of informal conversations. So anyone want to ask any questions?

Unknown Speaker  2:11:18  
Yeah, please their mics. That's right. There are mics in the front. We

Unknown Speaker  2:11:20  
have mics right here.

Unknown Speaker  2:11:27  
I'll be runners great.

Unknown Speaker  2:11:38  
Although

Unknown Speaker  2:11:39  
it's a simple question, would the

Unknown Speaker  2:11:41  
Jimmy diamond it's perfectly alright.

Unknown Speaker  2:11:45  
What was the simple question,

Unknown Speaker  2:11:46  
but Jimmy diamond and Ray Dalio persuaded Trump to shut down Bitcoin and us.

Unknown Speaker  2:11:51  
Um, my I mean, given given the the can't even get a Muslim ban working right, I think no.

Unknown Speaker  2:12:05  
In the US as a First Amendment, so that's effects cryptography at least. So thanks, thanks for your insights. Hey, guys, my question is for a Balaji, specifically. So when you think about cryptocurrency, you think of one of the benefits is like, it's it's decentralized, in the sense that its value doesn't really depend on the existence of any one company, organization. And then on. On the other hand, when you think of, of financial instruments like like equity or securities, they're valuable because they're, they're backed by real assets with with equity, it's like, the assets of a company. And with with a coin that's issued by a company, it kind of doesn't have either of these properties. It's not decentralized. And it's also not really backed by a real asset, except perhaps, than the network that the company has built around around it. And so if 21 went away with the 21 tokens still have value, do you expect its value to outlive the company and kind of streaming? how that works? Yeah,

Unknown Speaker  2:13:20  
yeah. So a couple of thoughts on that. So when we were thinking about what we're doing is, there were the three things that were previously distinct, which were like, let's say, you know, the equity of social network like Facebook, or LinkedIn, the internal currency of that network and API key to go and issue, you know, tasks, or what have you, or ads, etc, on that network. And so we've tried to do is combine all three of those. And the off chain value that you get, if what we're doing works, is the user database that you can script with that token. So that's the thing that actually would give it value in the sense of, you know, Bitcoin, or aetherium, our transaction database is not user databases, these are data is off chain, in the sense of it's held in exchanges, or it's held in other places. And that user data is important. Because without it, you know, there's a lot of things that you can't do programs, you can't write Sebby. First, the element that would give this value, there would be a database off chain that would give a value. And then second is in terms of, you know, if we vanish in a puff of smoke, you know, does the token value? Well, that's a really interesting experiment that we'll have to actually hopefully don't, we don't have to run it. But um, you know, one thing I noticed is that, you know, with Litecoin, or other kinds of coins like that, something that's really different for, you know, the coin space or the token space more generally, is that would like when the founder, basically, you know, Charlie was, you know, Mia for, you know, quite a long time until it kind of searched back, but it didn't go to zero. In fact, folks picked up the mantle and just kept developing it, so long as you have a token that's on a blockchain that, let's say millions or 10s of millions of people have, that itself has utility And it can be scripted. Essentially, the way that we're doing it is the only part that's really centralized is an issuance of it. And the reason that centralized is, if you're giving away free money, there has to be some gate of some kind. And, you know, what Satoshi did is he said, okay, that gate is going to be a computational gate, right? And what others have done is they said that gate is going to be a capital gate. Okay? So we're trying a third method, which is it's going to be a labor gate, but then once it's on the blockchain, then people can do whatever they want with it. And if it just becomes maybe the most ubiquitous coin, that may itself have value, because lots of folks have private keys associated with it. So that's kind of my, you know, somewhat like the answer to your question.

Unknown Speaker  2:15:37  
Okay. And I guess, to continue on the last point, just for clarification, will there be some use value

Unknown Speaker  2:15:44  
for the coin? Absolutely. Yeah. I'm sorry. Maybe I couldn't get to that. Yeah. No, it's just a pure Ponzi. Yeah, right. Yeah.

Unknown Speaker  2:15:51  
No, I mean, like the coins out there. But like, once a very good question. Yeah.

Unknown Speaker  2:15:55  
So the purpose of it is basically, to have something very similar. We've actually got this running, you can use it right now, it just uses Bitcoin, where you can buy replies from people, right. And that might seem like a very small thing. But LinkedIn in mail is like a 500 million a year business, where essentially, what a sender wants to do is they want to get in touch with somebody, say paid LinkedIn, and they put a message in that person's inbox that has three flaws. The first is that the recipient doesn't get paid. So you have no incentive to check your LinkedIn inbox. How many of you check your LinkedIn inbox today? Right? Number one, right? Number two, the sender, it cannot mass message people. Because LinkedIn knows it's a pain for them to message people. So they prohibit them from mass messaging. And number three, because they're imposing on somebody by sending them you know, a message, the sender can send a complex task. So we basically fix all three of those because the recipient gets paid, the sender can mass message, and the sender can send a complex task, like a survey, because they're attaching some money to it. So that's the fundamental utility of it is that with a social token, you can buy somebody time, you can ask one person or 10,000 people to do a task, like, you know, responding to your survey, or, you know, looking at your blog post or you know, auditing your solidity code, or what have you. That's all actually live right now, we, you know, we're at a multi million dollar annual run rate just on folks using it for Bitcoin stuff, or rather with Bitcoin is internal token. And then we're gonna replace it with our own token, which will allow every person who signs up to merely have some spare country to send with. So that's, that's the short version, it's a way to buy human time.

Unknown Speaker  2:17:20  
Okay, great. Thank you.

Unknown Speaker  2:17:23  
Hey, thanks, guys. My Mo, two questions. First question, it seems that on chain exchanges, like aren't going to work or like there's like a fundamental sort of like problem with them, right? Can you can plasma in a way sort of make on chain exchanges possible, or maybe some new construction blockchain construction that can make on chain sort of decentralized exchanges more feasible, you know, without the problems that we've seen?

Unknown Speaker  2:17:51  
Yeah. So um, yesterday in the presentation, I went to went over how to actually design conceptually, a decentralized exchange using like a MapReduce construction, essentially, instead of using, you know, the canonical word count example, you're, instead of combining words, you're combining things into a single order book. So instead of, you know, a matching set of words, who do the prices conceptually, and then just like, reduce it down and map it back up? So you can do an order book that way, but the primary role of centralized exchanges, is that the gateway, right, and that part is the tricky part. All right, assuming

Unknown Speaker  2:18:23  
we don't care about Fiat anymore, right? Like friggin Yes.

Unknown Speaker  2:18:26  
If your crypto to crypto exchanges. Yeah, you can do that.

Unknown Speaker  2:18:28  
There's is there still not the problem of like front running, though, and like sort of minor validation.

Unknown Speaker  2:18:33  
So you can run this? So I'm an advisor for a tokonoma, Seiko and the design for that one is uses batch execution. And you basically tag which block height you want to send your order into. And yes, other people can feel your order before then. I mean, not feel you're there, but also issue other orders before then. But that sort of doesn't matter, because the execution point happens at one point. So it's like you're willing to pay that price. Anyway.

Unknown Speaker  2:19:00  
Cool. Thanks. And second part, I came across this project recently,

Unknown Speaker  2:19:03  
guys, let's let's stick with one part question. Okay, let's do one one part. Yeah. keep

Unknown Speaker  2:19:06  
them short. Questions and Answers.

Unknown Speaker  2:19:09  
Great. So I have a general and I'm also a clicker. Balaji, a question on yours. So the specific question is, yeah, you mentioned Do you want people to help with this decentralize? You know, dashboard, and things like that. Right. So I'd love to hear more details of that. And specifically going on that right, thank you and metallic both touched on this point of the 90% things that you can do over 10% to kind of make this more user friendly, right. And so I'd like I'd love to hear your thoughts on what are those low hanging fruits that you guys see are standing in the way of user adoption right or user friendliness?

Unknown Speaker  2:19:40  
What are the low hanging fruit in terms of user friendliness, I would say on one important one is that we just need better multi SIG wallets. And we need them to the point where they're good enough that people can just use them as the default kind of wallet forever for anything. The And in general, I think, yeah, it's like user, just the user friendliness of security technology is specifically which has kind of a major. A major stumbling points at this point multi SIG being one, like, smart contract security being another Elad side of that, I would probably also say, mobile wallets, we probably need more iron, need more progress in also just Fiat Fiat to crypto gateways, maybe the first things that come to mind

Unknown Speaker  2:20:39  
about your question. So in terms of just comp, I agree with all the touch points, I'd say that, I think it's also going to be useful to try to make it so that getting cryptocurrency is as easy as signing up for a web app. And that onboarding problem is something which is at least what we're trying to do, so that you don't have to whip out a credit card or learn, you know, mining or what have you. And the second point regarding the decentralized approach, if anybody wants to build that, and you're a good web dev, basically just one, one person who will actually, you know, just do it, and I'll happily fund you just come up to me afterwards, or email me with like an example web app that you build, like, ideally, like a real time dashboard, and we'll pay you to do it, it'll be pretty cool.

Unknown Speaker  2:21:18  
We only have like three more minutes for questions, so we can speed it up.

Unknown Speaker  2:21:23  
Thank you guys. So in terms of aetherium, scaling, you know, 2000s, or 10,000 transaction, do you guys think that the ideas are mature enough that it's just you know, putting them into code will get us there or DC some fundamental problem that is still on the way of doing that.

Unknown Speaker  2:21:41  
And personally, I feel like we're kind of nearing the, the light at the end of the tunnel in terms of fundamental theoretical obstacles. But it's also important to really not underestimate the the level of kind of sheer like, you know, boring, long slog of just like practical code development and testing that will be required to put all of this into practice. So like, I could expect, kind of a lower and lower security versions of shorting to become available, like even sooner, and then a couple of years, but you know, it gets and you know, same like, same thing with other with other solutions. So like, for example, I know, ryden is like in the process of rolling of well announced since developer preview is in the process of rolling out, but you know, it will kind of get up to it. Okay, continue going off to full maturity over there. It'll take a year, I think quite get there.

Unknown Speaker  2:22:37  
Right. Thank you. Okay, last questions. Yeah. Great. Thank you guys.

Unknown Speaker  2:22:42  
You brought up twice already, like smart contracts and the user friendliness of those kind of things. There's been a lot of success with the ERC. 20. So I'm wondering if you're seeing any kind of advancement in other protocols that are coming into this space? And is that something that we should really be focusing on in order to kind of expand that? And if so, where are you seeing that happen? Like, I know there's opens up windows or any place else that might be working on those things?

Unknown Speaker  2:23:04  
So you're asking what things other than ERC 20 benefit from my standardization. Exactly.

Unknown Speaker  2:23:09  
Yeah. Like other any other kind of protocols?

Unknown Speaker  2:23:11  
Yeah. Okay. So one of the one kind of standard is just standards for anything that has to do with wallets and like, secure and security protocols. So you know, like messages of the forum, like, I authorize this, should I authorize this transaction, you know, like, I know, approvals like challenges, cancellations, just everything to do with like, making security policies that are more complex than one key decides everything. And so like, the reason why benefits aren't standardization is basically so the different wallets could run these and the different and so that different wallets could serve as Oracle's for things like, you know, like signing off on a transaction if they Yeah, verifying your identity or, or if you if they get a phone, like a cell phone confirmation code. And so that you could have these kind of that difficult, you could have separate marketplaces for providers of that and providers of wallet software. The Another thing is, that's where that's really where important standardize is Oracle's. So, you know, like, just getting information about the outside world. You know, a third is probably everything that has to do with identities. A fourth is just registries. So things like things like DNS, I mean, those are probably the ones that kind of most immediately come to mind. Do you know if there's

Unknown Speaker  2:24:31  
people collaborating on that, like me, like the open Zeppelin kind of?

Unknown Speaker  2:24:35  
I'm not sure if there are kind of major collaborations around yet. I think it's still due to early stage.

Unknown Speaker  2:24:43  
There's one thing that comes to mind, which is a recent WC payments API, I think so It's also worth thinking about.

Unknown Speaker  2:24:51  
Alright, okay, so we're at 630. So we're gonna break now and like I said, there's food outside we can have informal conversations.

Transcribed by https://otter.ai
